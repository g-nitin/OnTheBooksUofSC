{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Preferred notebook for model creation -- 100 trials give similar results as 500 trials; also used for label prediction (last run Nov 20, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This file creates model with MORE labelled data (Coded Sentences Drawn from Civic Center's Research and Taylor (1892_labeled_sentences.csv). Renamed \"Label\" column to \"jim_crow\".\n",
    "- Added 11 updated files with correct splitting and coded by Axton : 1868_24-25_updated, 1868_143-145_updated, 1868_146-151_updated, 1877_571-586_updated.csv, 1884_24-26_updated.csv, 1886-87_1031-1034_updated.csv, 1886-87_1072-1074_updated.csv,1868_24-25_updated, 1896_63act_updated, 1907_518-522_updated, 1925_324act_updated, shared_base_training_set_v2.csv (from UNC)\n",
    "- Converted the excel files coded by Axton to csv\n",
    "- Renamed the above files to make the naming consistent (- replaced by _, added 'labeled_sentences')\n",
    "- Renamed the column 'Coding (Axton)\" of Civic Center data to 'jim_crow' , 'Act' to 'sentence' and 'Year' to 'year' for consistency\n",
    "- Added column state = 'South Carolina' in all Civic Center coded files\n",
    "- Replaced the 'year' values in file 1886_87_1031-1034.csv from '1886-87' to 1886 and '1868_146-151_updated.csv' from '1868-69' to 1868\n",
    "- Changed case in some column names (upper to lower)\n",
    "- Read \"UNC_shared_base_training_set_v2.csv\" file differently to perform some filteration as mentioned by Matthew Jansen (UNC) \n",
    "- CASE 1: Training and testing model on combined data of UNC and USC\n",
    "- CASE 2: Training model on UNC data and testing model on USC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Used SMOTEN to rebalance the imbalanced dataset and removed stopwords but the model performance did not improve in comparison to without SMOTEN/stopword removal.**\n",
    "- **The time taken to create the model is very less than the time taken without SMOTEN/stopwords removal.**\n",
    "- **This code also predicts labels for the WHOLE CORPUS and for EACH YEAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Training and testing model on both USC and UNC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vandana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data reading libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#Saving model so that we don't have to run it every time we open this notebook\n",
    "import pickle \n",
    "\n",
    "# Text Libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords') \n",
    "\n",
    "# Machine Learning Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Model Tuning Libraries\n",
    "from hyperopt import hp, tpe, space_eval, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "#Evaluation Libraries\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer, f1_score\n",
    "\n",
    "#Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Supression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and store in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from the folder \"LabeledData\"\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for file in Path(\"LabeledData_UNC_USC_Nov2023\").glob('*.csv'):\n",
    "    if str(file).find(\"base\") != -1 :\n",
    "        df1 = pd.read_csv(file)       # reading the \"UNC_shared_base_training_set_v2.csv\" file and store it in dataframe df1\n",
    "    else: \n",
    "        df = pd.read_csv(file, usecols=['year', 'state','sentence', 'jim_crow'])\n",
    "        new_df = new_df.append(df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_section_id</th>\n",
       "      <th>source</th>\n",
       "      <th>law_type</th>\n",
       "      <th>UNC_training</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_sents</th>\n",
       "      <th>char_len</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>jim_crow</th>\n",
       "      <th>jim_crow_type</th>\n",
       "      <th>year</th>\n",
       "      <th>corpus_sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947_session laws_979_2</td>\n",
       "      <td>murray</td>\n",
       "      <td>session laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Subject only to restrictions and conditions no...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Powers and duties of Commission.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>The said Textbook Purchase and Rental Commissi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Acquire by contract, and/or purchase, such tex...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>and instructional supplies, which are, or may ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus_section_id  source      law_type  UNC_training  \\\n",
       "0  1947_session laws_979_2  murray  session laws           1.0   \n",
       "1   1935_public laws_423_3  murray   public laws           1.0   \n",
       "2   1935_public laws_423_3  murray   public laws           1.0   \n",
       "3   1935_public laws_423_3  murray   public laws           1.0   \n",
       "4   1935_public laws_423_3  murray   public laws           1.0   \n",
       "\n",
       "            state                                           sentence  \\\n",
       "0  NORTH CAROLINA  Subject only to restrictions and conditions no...   \n",
       "1  NORTH CAROLINA                   Powers and duties of Commission.   \n",
       "2  NORTH CAROLINA  The said Textbook Purchase and Rental Commissi...   \n",
       "3  NORTH CAROLINA  Acquire by contract, and/or purchase, such tex...   \n",
       "4  NORTH CAROLINA  and instructional supplies, which are, or may ...   \n",
       "\n",
       "   section_sents  char_len reviewer  jim_crow jim_crow_type  year  \\\n",
       "0            1.0     779.0      NaN         1      explicit  1947   \n",
       "1           10.0      32.0    Axton         0      explicit  1935   \n",
       "2           10.0     146.0    Axton         0      explicit  1935   \n",
       "3           10.0      53.0    Axton         0      explicit  1935   \n",
       "4           10.0     218.0    Axton         0      explicit  1935   \n",
       "\n",
       "  corpus_sentence_id  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read first 5 rows of \"UNC_shared_base_training_set_v2.csv\" file whose columns are different from other csv files\n",
    "df1[['jim_crow']] = df1[['jim_crow']].astype(int) # Change the datatype of jim_crow\" to int in df1 (it is 0.0, 1.0, 2.0 currently)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10231"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find length of df1\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only select rows which have a 'jim_crow' value of either 0 or 1\n",
    "df.loc[df.jim_crow.isin([0,1]), :]\n",
    "\n",
    "#### Only select rows which have a 'jim_crow_type' value of 'extrinsic' or 'implicit'\n",
    "df.loc[df.jim_crow_type.isin([\"extrinsic\",\"implicit\"]), :]\n",
    "\n",
    "#### Only select rows which DONT have a 'jim_crow_type' value of 'extrinsic' or 'implicit' by using '~'\n",
    "df.loc[~df.jim_crow_type.isin([\"extrinsic\",\"implicit\"]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_section_id</th>\n",
       "      <th>source</th>\n",
       "      <th>law_type</th>\n",
       "      <th>UNC_training</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section_sents</th>\n",
       "      <th>char_len</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>jim_crow</th>\n",
       "      <th>jim_crow_type</th>\n",
       "      <th>year</th>\n",
       "      <th>corpus_sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947_session laws_979_2</td>\n",
       "      <td>murray</td>\n",
       "      <td>session laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Subject only to restrictions and conditions no...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Powers and duties of Commission.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>The said Textbook Purchase and Rental Commissi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Acquire by contract, and/or purchase, such tex...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1935_public laws_423_3</td>\n",
       "      <td>murray</td>\n",
       "      <td>public laws</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>and instructional supplies, which are, or may ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>Axton</td>\n",
       "      <td>0</td>\n",
       "      <td>explicit</td>\n",
       "      <td>1935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus_section_id  source      law_type  UNC_training  \\\n",
       "0  1947_session laws_979_2  murray  session laws           1.0   \n",
       "1   1935_public laws_423_3  murray   public laws           1.0   \n",
       "2   1935_public laws_423_3  murray   public laws           1.0   \n",
       "3   1935_public laws_423_3  murray   public laws           1.0   \n",
       "4   1935_public laws_423_3  murray   public laws           1.0   \n",
       "\n",
       "            state                                           sentence  \\\n",
       "0  NORTH CAROLINA  Subject only to restrictions and conditions no...   \n",
       "1  NORTH CAROLINA                   Powers and duties of Commission.   \n",
       "2  NORTH CAROLINA  The said Textbook Purchase and Rental Commissi...   \n",
       "3  NORTH CAROLINA  Acquire by contract, and/or purchase, such tex...   \n",
       "4  NORTH CAROLINA  and instructional supplies, which are, or may ...   \n",
       "\n",
       "   section_sents  char_len reviewer  jim_crow jim_crow_type  year  \\\n",
       "0            1.0     779.0      NaN         1      explicit  1947   \n",
       "1           10.0      32.0    Axton         0      explicit  1935   \n",
       "2           10.0     146.0    Axton         0      explicit  1935   \n",
       "3           10.0      53.0    Axton         0      explicit  1935   \n",
       "4           10.0     218.0    Axton         0      explicit  1935   \n",
       "\n",
       "  corpus_sentence_id  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only select those rows which have value of either 0 or 1 in \"jim_crow\" column AND do not have value of \"extrinsic\" or \n",
    "# \"intrinsic\" in \"jim_crow_type\" column ---- as suggested by Matt Jansen (UNC)\n",
    "df1_good = df1.loc[(df1.jim_crow.isin([0,1])) & (~df1.jim_crow_type.isin([\"extrinsic\",\"implicit\"])),:].copy()\n",
    "df1_good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Subject only to restrictions and conditions no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1935</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Powers and duties of Commission.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1935</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>The said Textbook Purchase and Rental Commissi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1935</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>Acquire by contract, and/or purchase, such tex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1935</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>and instructional supplies, which are, or may ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10225</th>\n",
       "      <td>1909</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>On behalf of the general welfare of the city o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10226</th>\n",
       "      <td>1895</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>The clerk of the commissioners, on or before t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10228</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>The township school trustees shall divide thei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10229</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>They shall consult the convenience and necessi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>1901</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>That the Treasurer of Wilkes County be and he ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9492 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year           state  \\\n",
       "0      1947  NORTH CAROLINA   \n",
       "1      1935  NORTH CAROLINA   \n",
       "2      1935  NORTH CAROLINA   \n",
       "3      1935  NORTH CAROLINA   \n",
       "4      1935  NORTH CAROLINA   \n",
       "...     ...             ...   \n",
       "10225  1909  NORTH CAROLINA   \n",
       "10226  1895  NORTH CAROLINA   \n",
       "10228  1899  NORTH CAROLINA   \n",
       "10229  1899  NORTH CAROLINA   \n",
       "10230  1901  NORTH CAROLINA   \n",
       "\n",
       "                                                sentence  jim_crow  \n",
       "0      Subject only to restrictions and conditions no...         1  \n",
       "1                       Powers and duties of Commission.         0  \n",
       "2      The said Textbook Purchase and Rental Commissi...         0  \n",
       "3      Acquire by contract, and/or purchase, such tex...         0  \n",
       "4      and instructional supplies, which are, or may ...         0  \n",
       "...                                                  ...       ...  \n",
       "10225  On behalf of the general welfare of the city o...         0  \n",
       "10226  The clerk of the commissioners, on or before t...         1  \n",
       "10228  The township school trustees shall divide thei...         0  \n",
       "10229  They shall consult the convenience and necessi...         1  \n",
       "10230  That the Treasurer of Wilkes County be and he ...         1  \n",
       "\n",
       "[9492 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe from df1_good that has columns same as new_df\n",
    "new_df1 = df1_good[['year', 'state', 'sentence', 'jim_crow']]\n",
    "new_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be the duty of each county treasurer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>all moneys disbursed by any county treasurer o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>each county treasurer shall make out and forwa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>the county treasurer shall carry forward all s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be unlawful for any county treasurer,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year           state                                           sentence  \\\n",
       "0  1896  SOUTH CAROLINA  it shall be the duty of each county treasurer ...   \n",
       "1  1896  SOUTH CAROLINA  all moneys disbursed by any county treasurer o...   \n",
       "2  1896  SOUTH CAROLINA  each county treasurer shall make out and forwa...   \n",
       "3  1896  SOUTH CAROLINA  the county treasurer shall carry forward all s...   \n",
       "4  1896  SOUTH CAROLINA  it shall be unlawful for any county treasurer,...   \n",
       "\n",
       "   jim_crow  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 5 rows of the data new_df\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, state, sentence, jim_crow]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace '1886-87' in \"year\" column by 1886 and '1868-69' by 1868\n",
    "new_df.replace('1886-87', '1886', inplace=True)\n",
    "new_df.replace('1868-69', '1868', inplace=True)\n",
    "new_df[new_df['year'] == '1868-69'].head() # emplty dataframe because we have replaced \"1886-87\" by \"1886\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The length of new_df is 8978\n",
      " The length of new_df1 is 9492\n"
     ]
    }
   ],
   "source": [
    "# find the size of each dataframe\n",
    "print(\" The length of new_df is\", len(new_df))\n",
    "print(\" The length of new_df1 is\", len(new_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check how many entries are null or empty in the two datasets\n",
    "print(new_df.isnull().sum().sum())\n",
    "print(new_df1.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column datatypes of new_df are below: \n",
      " year        object\n",
      "state       object\n",
      "sentence    object\n",
      "jim_crow     int64\n",
      "dtype: object\n",
      "Column datatypes of new_df1 are below: \n",
      " year         int64\n",
      "state       object\n",
      "sentence    object\n",
      "jim_crow     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# find the data types of columns of new_df1 and new_df\n",
    "print(\"Column datatypes of new_df are below: \\n\", new_df.dtypes)\n",
    "print(\"Column datatypes of new_df1 are below: \\n\", new_df1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data type of the two dataframes are SAME as we are going to append the datasets to create ONE large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of \"year\" and \"jim_crow\" to string in new_df1 \n",
    "new_df1[['jim_crow']] = new_df1[['jim_crow']].astype(int)\n",
    "#new_df1[['year']] = new_df1[['year']].astype(str)\n",
    "\n",
    "# # Change the datatype of 'year' to string in new_df\n",
    "new_df[['year']] = new_df[['year']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column datatypes of new_df are below: \n",
      " year         int64\n",
      "state       object\n",
      "sentence    object\n",
      "jim_crow     int64\n",
      "dtype: object\n",
      "Column datatypes of new_df1 are below: \n",
      " year         int64\n",
      "state       object\n",
      "sentence    object\n",
      "jim_crow     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# find the data types of columns of new_df1 and new_df after changing\n",
    "print(\"Column datatypes of new_df are below: \\n\", new_df.dtypes)\n",
    "print(\"Column datatypes of new_df1 are below: \\n\", new_df1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1896, 1907, 1886, 1877, 1879, 1898, 1900, 1903, 1904, 1905, 1908,\n",
       "       1909, 1910, 1911, 1912, 1914, 1915, 1916, 1917, 1918, 1920, 1924,\n",
       "       1926, 1928, 1930, 1934, 1872, 1878, 1906, 1891, 1923, 1935, 1937,\n",
       "       1938, 1939, 1940, 1943, 1944, 1945, 1947, 1933, 1927, 1925, 1913,\n",
       "       1899, 1901, 1941, 1949, 1889, 1887, 1885, 1883, 1881, 1880, 1897,\n",
       "       1893, 1895, 1929, 1931, 1919, 1955, 1959, 1967, 1965, 1957, 1869,\n",
       "       1963, 1951, 1868, 1961, 1870, 1921, 1953, 1876, 1871, 1866, 1874,\n",
       "       1873, 1956, 1888, 1892, 1884])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.year.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the 2 datasets to create a big labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18470\n"
     ]
    }
   ],
   "source": [
    "# Create a large dataset with the 2 separate datasets\n",
    "combined_df = new_df.append(new_df1,ignore_index = True)\n",
    "\n",
    "#find the length of combined dataset\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined data to csv file\n",
    "combined_df.to_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be the duty of each county treasurer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>all moneys disbursed by any county treasurer o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>each county treasurer shall make out and forwa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>the county treasurer shall carry forward all s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be unlawful for any county treasurer,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18465</th>\n",
       "      <td>1909</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>on behalf of the general welfare of the city o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18466</th>\n",
       "      <td>1895</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>the clerk of the commissioners, on or before t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18467</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>the township school trustees shall divide thei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18468</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>they shall consult the convenience and necessi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18469</th>\n",
       "      <td>1901</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>that the treasurer of wilkes county be and he ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18470 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year           state  \\\n",
       "0      1896  SOUTH CAROLINA   \n",
       "1      1896  SOUTH CAROLINA   \n",
       "2      1896  SOUTH CAROLINA   \n",
       "3      1896  SOUTH CAROLINA   \n",
       "4      1896  SOUTH CAROLINA   \n",
       "...     ...             ...   \n",
       "18465  1909  NORTH CAROLINA   \n",
       "18466  1895  NORTH CAROLINA   \n",
       "18467  1899  NORTH CAROLINA   \n",
       "18468  1899  NORTH CAROLINA   \n",
       "18469  1901  NORTH CAROLINA   \n",
       "\n",
       "                                                sentence  jim_crow  \n",
       "0      it shall be the duty of each county treasurer ...         0  \n",
       "1      all moneys disbursed by any county treasurer o...         0  \n",
       "2      each county treasurer shall make out and forwa...         0  \n",
       "3      the county treasurer shall carry forward all s...         0  \n",
       "4      it shall be unlawful for any county treasurer,...         0  \n",
       "...                                                  ...       ...  \n",
       "18465  on behalf of the general welfare of the city o...         0  \n",
       "18466  the clerk of the commissioners, on or before t...         1  \n",
       "18467  the township school trustees shall divide thei...         0  \n",
       "18468  they shall consult the convenience and necessi...         1  \n",
       "18469  that the treasurer of wilkes county be and he ...         1  \n",
       "\n",
       "[18470 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert column 'sentence' to lowercase use map()\n",
    "combined_df['sentence'] = combined_df['sentence'].map(str.lower)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NORTH CAROLINA</th>\n",
       "      <td>14913</td>\n",
       "      <td>14913</td>\n",
       "      <td>14913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOUTH CAROLINA</th>\n",
       "      <td>3557</td>\n",
       "      <td>3557</td>\n",
       "      <td>3557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 year  sentence  jim_crow\n",
       "state                                    \n",
       "NORTH CAROLINA  14913     14913     14913\n",
       "SOUTH CAROLINA   3557      3557      3557"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of North Carolina and SOuth Carolina sentences\n",
    "count_df = combined_df.groupby('state').count()\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>sentence</th>\n",
       "      <th>jim_crow</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be the duty of each county treasurer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>all moneys disbursed by any county treasurer o...</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>each county treasurer shall make out and forwa...</td>\n",
       "      <td>0</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>the county treasurer shall carry forward all s...</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>it shall be unlawful for any county treasurer,...</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18465</th>\n",
       "      <td>1909</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>on behalf of the general welfare of the city o...</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18466</th>\n",
       "      <td>1895</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>the clerk of the commissioners, on or before t...</td>\n",
       "      <td>1</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18467</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>the township school trustees shall divide thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18468</th>\n",
       "      <td>1899</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>they shall consult the convenience and necessi...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18469</th>\n",
       "      <td>1901</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>that the treasurer of wilkes county be and he ...</td>\n",
       "      <td>1</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year           state  \\\n",
       "0      1896  SOUTH CAROLINA   \n",
       "1      1896  SOUTH CAROLINA   \n",
       "2      1896  SOUTH CAROLINA   \n",
       "3      1896  SOUTH CAROLINA   \n",
       "4      1896  SOUTH CAROLINA   \n",
       "...     ...             ...   \n",
       "18465  1909  NORTH CAROLINA   \n",
       "18466  1895  NORTH CAROLINA   \n",
       "18467  1899  NORTH CAROLINA   \n",
       "18468  1899  NORTH CAROLINA   \n",
       "18469  1901  NORTH CAROLINA   \n",
       "\n",
       "                                                sentence  jim_crow  \\\n",
       "0      it shall be the duty of each county treasurer ...         0   \n",
       "1      all moneys disbursed by any county treasurer o...         0   \n",
       "2      each county treasurer shall make out and forwa...         0   \n",
       "3      the county treasurer shall carry forward all s...         0   \n",
       "4      it shall be unlawful for any county treasurer,...         0   \n",
       "...                                                  ...       ...   \n",
       "18465  on behalf of the general welfare of the city o...         0   \n",
       "18466  the clerk of the commissioners, on or before t...         1   \n",
       "18467  the township school trustees shall divide thei...         0   \n",
       "18468  they shall consult the convenience and necessi...         1   \n",
       "18469  that the treasurer of wilkes county be and he ...         1   \n",
       "\n",
       "       sent_length  \n",
       "0              533  \n",
       "1              249  \n",
       "2              808  \n",
       "3              320  \n",
       "4              318  \n",
       "...            ...  \n",
       "18465          365  \n",
       "18466          477  \n",
       "18467          136  \n",
       "18468          282  \n",
       "18469          336  \n",
       "\n",
       "[18470 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find length of each sentence\n",
    "combined_df['sent_length'] = combined_df['sentence'].str.len()\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The longest sentence is of length: 8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([2983, 10561], dtype='int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max and min length of the sentence\n",
    "print(\" The longest sentence is of length:\", combined_df['sent_length'].max())\n",
    "\n",
    "# find the index of the row with longest sentence\n",
    "combined_df.index[combined_df['sent_length'] == 8889]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                                                        1909\n",
       "state                                             NORTH CAROLINA\n",
       "sentence       all laws and clauses of laws in conflict with ...\n",
       "jim_crow                                                       0\n",
       "sent_length                                                   75\n",
       "Name: 5298, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[5298,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2aab4049c590>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "# Histogram for the sentence length\n",
    "sns.histplot(data=df, x=combined_df['sent_length'], bins = 50, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows whose sentence length is less than 27 words\n",
    "combined_df_27char = combined_df[combined_df['sent_length'] < 27]\n",
    "#combined_df_27char.to_csv(\"combined_df_27char.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of sentences less than 25 characters are 463 and they are all labeled as 0 except one (labeled 2). So, we can delete all senetences whose length are less than 25.\n",
    "- The number of sentences less than 27 characters are 549 and they are all labeled as 0 except three (one labeled 1, two lableled 2). Since we do not want to delete any sentence coded as 1, we will delete all sentences less than 27 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all the sentences with < 27 characters. Create dataset with >= 27 characaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows in the reduced dataset: 17972\n"
     ]
    }
   ],
   "source": [
    "# Remove all the sentences whose length is <= characters\n",
    "df_updated = combined_df[combined_df['sent_length'] >= 27]\n",
    "print(\" Number of rows in the reduced dataset:\", len(df_updated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year            int64\n",
       "state          object\n",
       "sentence       object\n",
       "jim_crow        int64\n",
       "sent_length     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the data type of reduced dataframe columns\n",
    "df_updated.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences that are NOT jim crow = 13223\n",
      "Number of jim crow sentences = 4100\n",
      "Number of undecided sentences = 649\n"
     ]
    }
   ],
   "source": [
    "# Find the number of sentences with labels \"0\", \"1\", and \"2\"\n",
    "df_0 = df_updated[df_updated['jim_crow'] == 0]\n",
    "df_1 = df_updated[df_updated['jim_crow'] == 1]\n",
    "df_2 = df_updated[df_updated['jim_crow'] == 2]\n",
    "\n",
    "print(\"Number of sentences that are NOT jim crow =\", len(df_0))\n",
    "print(\"Number of jim crow sentences =\", len(df_1))\n",
    "print(\"Number of undecided sentences =\", len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year            int64\n",
       "state          object\n",
       "sentence       object\n",
       "jim_crow        int64\n",
       "sent_length     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2aab404aa2d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seaborn import countplot\n",
    "\n",
    "# Plot the distribution of the target (=1 that is jim_crow)\n",
    "countplot(x=df_updated['jim_crow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df_updated.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords from the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words(\"english\")\n",
    "\n",
    "# Define a function to remove stop words from a sentence \n",
    "def remove_stop_words(sentence): \n",
    "  # Split the sentence into individual words \n",
    "    words = sentence.split() \n",
    "  \n",
    "  # Use a list comprehension to remove stop words \n",
    "    filtered_words = [word for word in words if word not in stopWords] \n",
    "  \n",
    "  # Join the filtered words back into a sentence \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-895e57d0d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_updated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_updated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_updated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2deeaf2621e7>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Use a list comprehension to remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Join the filtered words back into a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2deeaf2621e7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Use a list comprehension to remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Join the filtered words back into a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove stopwords from the 'sentence' column of the dataframe\n",
    "p=[]\n",
    "for i in df_updated['sentence']:\n",
    "    p.append(remove_stop_words(i))\n",
    "df_updated['sentence']=p\n",
    "df_updated['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into 'features' and 'target' for machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "features = df_updated[['year', 'sentence']]\n",
    "target = df_updated['jim_crow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(features.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting test and training set in 20|80 ratio\n",
    "- Using default values of the parameter\n",
    "- Training and test set contains mixed of NC and SC sentences\n",
    "- Resample training data using imblearn SMOTEN as the dataset is very imbalanced - using SMOTEN because our dataset is purely categorical (https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.html#imblearn.over_sampling.SMOTEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomly split X and y into training (80%) and test(20%) sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"......TRAINING SET....\")\n",
    "print(\" Size of X_train is: \", X_train.shape)\n",
    "print(\" Size of y_train is: \", y_train.shape)\n",
    "print(\"......TEST SET....\")\n",
    "print(\" Size of X_test is: \", X_test.shape)\n",
    "print(\" Size of y_test is: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of 0,1,2 in training set BEFORE resampling using SMOTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting 0,1,2 in y_train \n",
    "np.array(np.unique(y_train, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the data using SMOTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTEN(random_state=13)\n",
    "X_train, y_train = sm.fit_resample(X_train, np.array(y_train))\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of 0,1,2 in training set AFTER resampling using SMOTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of 0,1,2 after using SMOTEN\n",
    "np.array(np.unique(y_train, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LengthExtractor(BaseEstimator, TransformerMixin):   \n",
    "#     def compute_length(self, text):\n",
    "#         sentence_list = word_tokenize(text)\n",
    "#         return len(sentence_list) \n",
    "#     def fit(self, x, y=None):\n",
    "#         return self\n",
    "#     def transform(self, X):\n",
    "#         X_length = pd.Series(X).apply(self.compute_length)\n",
    "#         return pd.DataFrame(X_length)\n",
    "\n",
    "class SelectColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "    def transform(self, X, **transform_params):\n",
    "        out = X[self.columns].copy()\n",
    "        return out\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Bayesian Hyperparameter search with HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            # Extract features\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('get_text', SelectColumnsTransformer(\"sentence\")),\n",
    "                ('vect', CountVectorizer(decode_error = \"ignore\",\n",
    "                      min_df = 2, max_df = 0.8)),\n",
    "                ('tfidf', 'passthrough'),\n",
    "            ])),\n",
    "#             ('text_len', Pipeline([\n",
    "#                 ('get_text', SelectColumnsTransformer(\"sentence\")),\n",
    "#                 ('length', LengthExtractor())\n",
    "#             ])),\n",
    "            ('metadata', SelectColumnsTransformer(['year']))\n",
    "        ])),\n",
    "        ('dlf',XGBClassifier(tree_method=\"hist\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params['features__text_pipeline__vect__min_df'] = int(params['features__text_pipeline__vect__min_df'])\n",
    "    params['dlf__max_depth'] = int(params['dlf__max_depth'])\n",
    "    params['dlf__min_child_weight'] = int(params['dlf__min_child_weight'])\n",
    "    params['dlf__scale_pos_weight'] = int(params['dlf__scale_pos_weight'])\n",
    "    pipeline.set_params(**params)\n",
    "    \n",
    "    score = 1-cross_val_score(pipeline, X_train, y_train, scoring=make_scorer(f1_score, average=\"weighted\"),\n",
    "                              n_jobs=-1).mean()\n",
    "    #print(\"F1 {:.4f} params {}\".format((1-score), params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'features__text_pipeline__vect__min_df': hp.quniform ('features__text_pipeline__vect__min_df',2,20,1),\n",
    "    'features__text_pipeline__vect__lowercase': hp.choice('features__text_pipeline__vect__lowercase',[True,False]),\n",
    "    'features__text_pipeline__vect__ngram_range': hp.choice('features__text_pipeline__vect__ngram_range',[(1,1),(1,2),(1,3),(1,4)]),\n",
    "    'features__text_pipeline__vect__max_df': hp.uniform('features__text_pipeline__vect__max_df',0.6,0.8),\n",
    "#     'features__text_len':hp.choice('features__text_len',[Pipeline([\n",
    "#             ('get_text', SelectColumnsTransformer(\"sentence\")),\n",
    "#             ('length', LengthExtractor())\n",
    "#         ])]),\n",
    "    'dlf__max_depth': hp.quniform('dlf__max_depth', 2, 10, 1),\n",
    "    'dlf__min_child_weight': hp.quniform('dlf__min_child_weight', 1, 5, 1),\n",
    "    'dlf__colsample_bytree': hp.uniform('dlf__colsample_bytree', 0.3, 1.0),\n",
    "    'dlf__learning_rate': hp.uniform('dlf__learning_rate', 0.05, 1),\n",
    "    'dlf__scale_pos_weight': hp.quniform('dlf__scale_pos_weight',1,8,1),\n",
    "    'dlf__gamma': hp.uniform('dlf__gamma',0,2)\n",
    "    \n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100, #actual fit was run with 1000 evals\n",
    "           trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = space_eval(space, best)\n",
    "model_params['features__text_pipeline__vect__min_df'] = int(model_params['features__text_pipeline__vect__min_df'])\n",
    "model_params['dlf__max_depth'] = int(model_params['dlf__max_depth'])\n",
    "model_params['dlf__min_child_weight'] = int(model_params['dlf__min_child_weight'])\n",
    "# model_params['dlf__scale_pos_weight'] = int(model_params['dlf__scale_pos_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the model so that we don't have to run the whole file whenever we want to predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = pipeline\n",
    "best_model.set_params(**model_params)\n",
    "best_model.fit(X_train,y_train)\n",
    "\n",
    "#pickle the best model\n",
    "pickle.dump(best_model, open('best_model.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SelectColumnTransformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d620f904cb6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSelectColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SelectColumnTransformer'"
     ]
    }
   ],
   "source": [
    "import SelectColumnTransformer\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_model = pickle.load(open('best_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of test data \n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# count the number of labels\n",
    "labels = np.unique(y_pred)\n",
    "\n",
    "data = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "# use sns.heatmap on top of confusion_matrix to show the confusion matrix\n",
    "sns.set(font_scale=1.4)\n",
    "ax = sns.heatmap(df_cm,xticklabels=True, annot=True, fmt='.0f', center=0.8, cmap='crest', linewidth=.5)\n",
    "ax.set(title=\"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict \"label\" for the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('../Split_Cleanup_Updated/updated_results/final_splits_Nov3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column 'id' at '_'\n",
    "corpus['year']=corpus['id'].str.split('_')\n",
    "corpus.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = []\n",
    "for i in corpus['year']:\n",
    "        id.append(i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id))\n",
    "print(type(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(set(id))))\n",
    "list(set(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_new=[]\n",
    "for i in id:\n",
    "    if i.find('-') != -1:\n",
    "        year = i.split('-')[0]\n",
    "        id_new.append(year)\n",
    "    else:\n",
    "        id_new.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(id_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace the 2-year pairs with single years:\n",
    "- '1868-69' --> 1868\n",
    "- '1869-1870' --> 1869\n",
    "- '1870-1871' --> 1870\n",
    "- '1871-1872' --> 1871\n",
    "- '1872-1873' --> 1872\n",
    "- '1873-1874' --> 1873\n",
    "- '1875-76'   --> 1875\n",
    "- '1877-78'   --> 1877\n",
    "- '1881-82'   --> 1881\n",
    "- '1886-1887' --> 1886\n",
    "- '1958b'     --> 1958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_df['year'] = id_new\n",
    "# ct_year = corpus_df['year'].unique()\n",
    "# print(ct_year)\n",
    "# print(len(ct_year))\n",
    "\n",
    "corpus['year'] = id_new\n",
    "ct_year = corpus['year'].unique()\n",
    "print(ct_year)\n",
    "print(len(ct_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'year' 1958b to 1958\n",
    "# corpus_df.replace('1958b', '1958', inplace=True)\n",
    "corpus.replace('1958b', '1958', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus_df['year'].unique())\n",
    "# print(len(corpus_df['year'].unique()))\n",
    "\n",
    "print(corpus['year'].unique())\n",
    "print(len(corpus['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_ml = corpus_df[['year', 'sentence']]\n",
    "# X_test_corpus.head()\n",
    "\n",
    "X_test_corpus = corpus[['year', 'sentence']]\n",
    "X_test_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_corpus.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of \"year\" and \"jim_crow\" to string in new_df1 \n",
    "X_test_corpus[['sentence']] = X_test_corpus[['sentence']].astype(str)\n",
    "X_test_corpus[['year']] = X_test_corpus[['year']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column 'sentence' to lowercase use map()\n",
    "X_test_corpus['sentence'] = X_test_corpus['sentence'].map(str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_ml = X_test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corpus=[]\n",
    "\n",
    "# Remove stopwords from the 'sentence' column of the dataframe\n",
    "new_sentences=[]\n",
    "for i in X_test_corpus['sentence']:\n",
    "    p_corpus.append(remove_stop_words(i))\n",
    "X_test_corpus['sentence'] = p_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict the labels of test data \n",
    "# y_pred_corpus = best_model.predict(X_test_ml)\n",
    "\n",
    "# # count the number of labels\n",
    "# corpus_df['predicted_label'] = y_pred_corpus\n",
    "\n",
    "#predict the labels of test data \n",
    "y_pred_corpus = best_model.predict(X_test_corpus)\n",
    "\n",
    "# # count the number of labels\n",
    "# corpus_df['predicted_label'] = y_pred_corpus\n",
    "\n",
    "# count the number of labels\n",
    "corpus['predicted_label'] = y_pred_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the corpus with predicted data in a csv file\n",
    "corpus.to_csv(\"Predicted_Data_Labels/PredLabels_corpus.csv\")\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, g in corpus.groupby('year'):\n",
    "    print('i is', i)\n",
    "    print(\"g is\", g)\n",
    "    df=g\n",
    "    df.to_csv(f'Predicted_Data_Labels/{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_by_year = corpus.groupby('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = dict(list(corpus_by_year))\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in groups.items():\n",
    "    val.to_csv(f'Predicted_Data_Labels/predicted_{key}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
