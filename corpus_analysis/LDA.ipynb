{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2dc01e-b715-4e1e-b9be-43fe4fee26f8",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e406ea3-08cd-4cf2-9621-548a57d23b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T21:30:34.321972Z",
     "start_time": "2023-10-12T21:30:34.232690Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "249c02a5-f186-4224-a9ed-44e50daaaf1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be952bed-b74f-4615-a66d-69b07bf362a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac934c1b-71ca-4288-b7f8-9473df37f773",
   "metadata": {},
   "source": [
    "## Data Acquisition and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d851cb0-c7da-4d9c-9217-af5ad82da35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1873-1874', '1892', '1893', '1894', '1901', '1918', '1921', '1928', '1948', '1956']\n"
     ]
    }
   ],
   "source": [
    "# Read all folder names in the OCR (or a specified) directory\n",
    "# ocred_path = '/work/otb-lab/OCRed'\n",
    "ocred_path = '/Users/nitingupta/Desktop/OTB/OCRed'\n",
    "\n",
    "years = [name for name in os.listdir(ocred_path) if not name.startswith('.')]\n",
    "years.sort()\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e2ed15-b715-4373-8038-c5700e6057ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Splitting/final-update/final-splits/final_splits.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Splitting/final-update/final-splits/final_splits.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/OTB/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/OTB/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/OTB/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/OTB/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/OTB/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Splitting/final-update/final-splits/final_splits.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Splitting/final-update/final-splits/final_splits.csv', index_col = 0, usecols=['id', 'sentence'])\n",
    "# df = pd.read_csv('../split/updated/results/final_splits.csv', index_col = 0, usecols=['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3afab-3a46-44fd-bcb8-ce901f381111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['year'] = df.index.str.split(\"_\").str[0]\n",
    "df.set_index('year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83add712-55cd-4fdb-83cb-ad76650028f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f90b9a-ef3d-499e-94f2-2d1736c8c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add some custom words to the list\n",
    "stop_words.append('said')\n",
    "stop_words.append('shall')\n",
    "stop_words.append('ee')\n",
    "stop_words.append('00')\n",
    "stop_words.append('state')\n",
    "stop_words.append('may')\n",
    "stop_words.append('src')\n",
    "stop_words.append('sec')\n",
    "stop_words.append('sec.')\n",
    "stop_words.append('town')\n",
    "stop_words.append('section')\n",
    "stop_words.append('county')\n",
    "stop_words.append('act')\n",
    "stop_words.append('board')\n",
    "stop_words.append('000')\n",
    "stop_words.append(';')\n",
    "stop_words.append('approved')\n",
    "stop_words.append('one')\n",
    "stop_words.append('general')\n",
    "stop_words.append('upon')\n",
    "stop_words.append('hereby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b846c4-e957-447c-9131-24c28fee8a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5bd8d-d0fd-4d9a-9316-cabf73310d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(sentence):\n",
    "    \"\"\"\n",
    "    Perform a basic cleaning that includes:\n",
    "        - Hyphen removals from words appeared at the end of a sentence and were split to the next line.\n",
    "        - Lowercasing\n",
    "        - Tokenization\n",
    "        - Removal of words that do not exclusively contain letters\n",
    "        - Removing stopwords\n",
    "        - Lemmatization\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyphen removal\n",
    "    sentence = re.sub(r'(—|_|-)( )*', '', sentence)\n",
    "    \n",
    "    # Lowercase and tokenize\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Keep only letters\n",
    "    words_alpha = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Stopword Removal\n",
    "    filtered_tokens = [word for word in words_alpha if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dea4fe-216c-4568-8add-a073a9ab02a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_sent'] = df['sentence'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec3104-5270-4e41-8441-9dbffc64661c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_sent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3ac4f-6c44-4dfe-9375-4830439527e7",
   "metadata": {},
   "source": [
    "### Removing Rare Words\n",
    "Filter out extreme words that won't be helpful in the topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea2560-167a-464c-bf8c-43d568dcdbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A list of all cleaned words from the corpus\n",
    "allwords = []\n",
    "df['cleaned_sent'].apply(lambda x: [allwords.append(word) for word in x])\n",
    "len(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711b4a2-360a-447e-a69f-091dc259bb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count word frequencies across the entire corpus\n",
    "word_counts = Counter(allwords)\n",
    "\n",
    "# Frequency threshold for rare words\n",
    "frequency_threshold = 10\n",
    "\n",
    "# Remove rare words\n",
    "df['cleaned_sent'] = df['cleaned_sent'].apply(lambda x: [word for word in x if word_counts[word] >= frequency_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337ca78-4c9e-4125-b826-2619556f7ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad4646e-b852-4154-bd0a-5eed589a5180",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8a3b-570c-4737-a87d-fd7d41e1ddd3",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582d78a-cab5-4b79-b3df-b91817945f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df['cleaned_sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80049f35-aac6-4e46-9d2b-a77d6d8ebaf7",
   "metadata": {},
   "source": [
    "### LDA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116762f-944a-43c0-a01e-3144080f2cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary and a corpus (Bag of Words)\n",
    "id2word = corpora.Dictionary(df['cleaned_sent'])  # a mapping between words and their integer ids\n",
    "corpus = [id2word.doc2bow(text) for text in df['cleaned_sent']]  # Convert document into the bag-of-words format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c94802-8c34-4487-b672-d54d05856fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word=id2word,\n",
    "                                     num_topics=10,\n",
    "                                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589bc59-4cae-4ba7-8d70-e7b01207d35b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topic Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b404b-183b-4164-aa46-1725ade895cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the topics and associated words\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a068ec-202c-42ef-bd0b-284ad944c76f",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca72b62-f1a7-4c17-afad-1585febdb628",
   "metadata": {},
   "source": [
    "Two evaluation metrics to consider for topic modeling: perplexity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53af5bf-8c14-4275-9ab1-2df3d419e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec327fc-1d31-4c19-bf46-3ae56f502e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity (lower is better)\n",
    "print('Perplexity:', round(lda_model.log_perplexity(corpus), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caa65d-74b2-4888-b6e9-aa00b98b8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score (higher is better)\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df['cleaned_sent'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score:', round(coherence_lda, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949ef67-2593-459e-8677-473de6cbea6d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bd64f-3b92-4037-9d64-74644d0ab7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74249c52-b07c-4c36-b500-6797be1d9c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c421be3-abe0-46f0-97dc-c72cfb46a28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2e4a910-94ea-40e5-9396-41281516bcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd19e8cb-fb15-4b4b-bd20-54b99e0f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
