{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2dc01e-b715-4e1e-b9be-43fe4fee26f8",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bb9cb-b48e-41ba-b214-2c908cc205ae",
   "metadata": {},
   "source": [
    "This notebook performs LDA on the SC law corpus.\n",
    "\n",
    "Topic modeling is a method for finding common themes from unstructured text and is useful in giving a comprehensive overview. Unlike traditional clustering, it embraces mixed membership, recognizing that data points can belong to multiple categories.\n",
    "\n",
    "Consider a scenario where we manually create topics for a collection of cooking recipes:\n",
    "- (Italian Cuisine) 50% \"pasta,\" 25% \"tomato,\" 10% \"olive oil,\" 5% \"basil\"...\n",
    "- (Baking) 65% \"flour,\" 12% \"sugar,\" 10% \"yeast,\" 5% \"vanilla\"...\n",
    "- (Healthy Eating) 42% \"vegetables,\" 15% \"quinoa,\" 7% \"salad,\" 7% \"nuts\"...\n",
    "\n",
    "These words are associated with their probabilities of appearing in the topic to the left. So, there is 50% chance that the word \"pasta\" came from the Italian Cuisine topic. Note that the vocabulary probability will sum up to 1 for every topic, but often times, words that have lower weights will be truncated from the output.\n",
    "\n",
    "The same concept extends to documents, allowing automated extraction of topics and document probabilities without manual labeling, exemplifying unsupervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458a702-9bb4-4d23-91b6-8c397121f62a",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a popular topic modeling algorithm and requires documents to be represented as a bag of words. This representation ignores word ordering in the document but retains information on how many times each word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed3f6b-8c32-4a6f-8e79-b46a5b433386",
   "metadata": {},
   "source": [
    "<br>Source(s) used:\n",
    "* https://doi.org/10.17615/tksc-t217\n",
    "* https://ethen8181.github.io/machine-learning/clustering/topic_model/LDA.html\n",
    "* https://medium.com/towards-data-science/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "* https://towardsdatascience.com/nlp-preprocessing-and-latent-dirichlet-allocation-lda-topic-modeling-with-gensim-713d516c6c7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e406ea3-08cd-4cf2-9621-548a57d23b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T21:58:48.137741Z",
     "start_time": "2023-12-15T21:58:46.414815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter\n",
    "import pickle\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from typing import List, Set, Dict\n",
    "\n",
    "# Since importing hyperopt and pyLDAvis might generate deprecation warnings\n",
    "# Filter those warnings here.\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    from hyperopt import fmin, tpe, hp\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6 \n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48f1ec-d041-4c69-abf6-d60746b86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%watermark --iso8601 -u -v -m -iv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb4f38-9045-4801-be0d-6dc87621d29b",
   "metadata": {},
   "source": [
    "<b>Note: </b>Hyperopt does not (at the time of writing this notebook) work with Python version 3.12. For this reason, it is recommended to use an eariler version of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be952bed-b74f-4615-a66d-69b07bf362a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac934c1b-71ca-4288-b7f8-9473df37f773",
   "metadata": {},
   "source": [
    "## Data Acquisition and Pre-Processing\n",
    "Get the data file and read all the sentences.\n",
    "<br>\n",
    "To avoid pre-processing the sentences repeatedly when rerunning the notebook, save them on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d630d6-0759-4644-815b-21d7e3cf385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences() -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load the split sentences and return a Pandas Series for those sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path to the final split csv\n",
    "    f_path: str = \"/Users/nitingupta/Desktop/OTB/final_splits_Nov3.csv\"\n",
    "    \n",
    "    # Use only the id (containing the years) and the sentences column\n",
    "    df: pd.DataFrame = pd.read_csv(f_path, index_col = 0, usecols=['id', 'sentence'])\n",
    "    \n",
    "    # Get the years\n",
    "    df['year']: pd.Series = df.index.str.split(\"_\").str[0]\n",
    "    df.set_index('year', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Convert to Series to get the 'sentence' column as a string type\n",
    "    sentences: pd.Series = pd.Series(df['sentence'], dtype=\"string\")\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5bd8d-d0fd-4d9a-9316-cabf73310d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean(sentence : str, stop_words: Set[str], lemmatizer: WordNetLemmatizer) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform a basic cleaning on the given sentences.\n",
    "    Cleaning includes:\n",
    "        - Hyphen removals from words that appeared at the end of a sentence and were split to the next line.\n",
    "        - Lowercasing\n",
    "        - Tokenization\n",
    "        - Removal of words that do not exclusively contain letters\n",
    "        - Removing stopwords\n",
    "        - Lemmatization\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyphen removal\n",
    "    sentence = re.sub(r'(â€”|_|-)( )*', '', str(sentence))\n",
    "    \n",
    "    # Lowercase and tokenize\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Keep only letters\n",
    "    words_alpha = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Stopword Removal\n",
    "    filtered_tokens = [word for word in words_alpha if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6a6be-9339-4902-930a-f3db46133681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentences: pd.Series) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Perform pre-processing on the given `sentences`.\n",
    "    Utilize the `clean` function defined above.\n",
    "    Also, convert potential words to bigrams.\n",
    "    Return a List of List of Strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load in the stopwords from the NLTK library\n",
    "    stop_words: List[str] = stopwords.words('english')\n",
    "    \n",
    "    # Add custom stop words fromt the `custom_stopwords.txt` file\n",
    "    # This is done to remove corpus sepcific words.\n",
    "    # Most of these custom words were brought from Dalwadi's paper (see sources).\n",
    "    with open('stopwords/custom_stopwords.txt', 'r') as f:\n",
    "        stop_words += f.read().splitlines()\n",
    "    \n",
    "    # Convert to a set for faster computations\n",
    "    stop_words: Set[str] = set(stop_words)\n",
    "    \n",
    "    print(f\"Number of stop words: {len(stop_words)}\")\n",
    "\n",
    "    # Initialize the WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        # Apply the \"clean\" function to each sentence\n",
    "        cleaned_sents: pd.Series = sentences.swifter.apply(lambda x: clean(x, stop_words, lemmatizer))\n",
    "\n",
    "    # Convert potential words to bigrams.\n",
    "    bigram = Phrases(cleaned_sents, min_count=5, threshold=100)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "\n",
    "    cleaned_sents: List[List[str]] = [bigram_mod[text] for text in cleaned_sents.tolist()]\n",
    "\n",
    "    return cleaned_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9b3cd-22f3-4b8b-96b4-9e98609fa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sents_path = 'cleaned_sentences.pkl'\n",
    "\n",
    "if os.path.exists(cleaned_sents_path):\n",
    "    print(f\"File at {cleaned_sents_path} exists! Reading from file.\")\n",
    "    \n",
    "    with open(cleaned_sents_path, 'rb') as f:\n",
    "        cleaned_sents: List[List[str]] = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(f\"File at {cleaned_sents_path} does not exists! Performing calculations.\")\n",
    "    \n",
    "    cleaned_sents: List[List[str]] = pre_processing(load_sentences())\n",
    "\n",
    "    print(f\"Saving cleaned senteces at {cleaned_sents_path}.\")\n",
    "    # Save\n",
    "    with open(cleaned_sents_path, 'wb') as f:\n",
    "        pickle.dump(cleaned_sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87aba1-e76c-4557-8661-07885e2f771d",
   "metadata": {},
   "source": [
    "Cleaned sentences is a List of List of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d5aa5-b8da-43fe-a1d3-5ea1318aea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of cleaned sentences: {len(cleaned_sents):,}\")\n",
    "print(f\"Sample output: {cleaned_sents[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d5269-e7a3-41b7-a2fb-31ef7b3dd9c5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3ac4f-6c44-4dfe-9375-4830439527e7",
   "metadata": {},
   "source": [
    "### Visualizing Words\n",
    "Before making the LDA model, it will be helpful to visualize the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea2560-167a-464c-bf8c-43d568dcdbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A list of all cleaned words from the corpus\n",
    "allwords: List[str] = []\n",
    "allwords = [x for xs in cleaned_sents for x in xs]\n",
    "print(f\"Total words in the corpus (after cleaning): {len(allwords):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f78cc6-e3b3-497a-86d3-1c2d03516c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies across the entire corpus\n",
    "word_counts: Counter = Counter(allwords)  # Dictionary containing word -> frequency\n",
    "print(f\"Number of unique words in the corpus (after cleaning): {len(word_counts):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a0048-c429-469f-8666-d831b7be48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe to get the plots and descriptive statistics\n",
    "df_word_counts: pd.DataFrame = pd.DataFrame.from_dict(word_counts, orient='index', columns=['frequency'])\n",
    "df_word_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843806c-a27f-4b6b-b769-b9d3d2d91e39",
   "metadata": {},
   "source": [
    "Many words have similar frequencies, as shown with the statistics above, and most of them are in the lower frequency range.\n",
    "<br>For example, 50% of all unique words occur only once.\n",
    "<br>So, plot the unique frequencies across the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb37fd6-72c5-4d70-8a3b-d871585a68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(sorted(list(set(word_counts.values()))))\n",
    "# plt.plot(sorted(word_counts.values()))\n",
    "\n",
    "# Reformat x-axis label and tick labels\n",
    "ax.set_xlabel('Number of Unique Frequencies', fontsize=12, labelpad=10)  # No need for an axis label\n",
    "ax.xaxis.set_label_position(\"bottom\")\n",
    "ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=12, labelrotation=0)\n",
    "\n",
    "# Reformat y-axis\n",
    "ax.set_ylabel('Frequency', fontsize=12, labelpad=10)\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "# Remove the spines\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "# Add in title\n",
    "ax.set_title(\"Unique Word Frequencies\", weight='bold')\n",
    "\n",
    "# Set a white background\n",
    "fig.patch.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53260682-f7eb-4f02-bc2d-d5411b6c6ba8",
   "metadata": {},
   "source": [
    "The plot suggests that most of the words occur a low number of times, and only a few ones have a high number of occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db434ee-0593-4ce8-b475-ad7c82aa33d9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8a3b-570c-4737-a87d-fd7d41e1ddd3",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Use the `Gensim` library to make the model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb3cda-28a8-43e2-b87d-e3c96d9cdcee",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582d78a-cab5-4b79-b3df-b91817945f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary: a mapping between words and their integer ids\n",
    "dictionary = corpora.Dictionary(cleaned_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b03011-5502-46c7-aebf-0fa9dd2584c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of the dictionary before filtering: {len(dictionary):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90ba3f-215d-4307-bed4-06d35b3c9df4",
   "metadata": {},
   "source": [
    "Filter out extreme words that won't be helpful in the topic modeling.\n",
    "<br>While removing stop words, previously, took care of the common words, words that are too rare (to constitute a meaningful topic or too common to add any meaning) were not considered.\n",
    "<br>Eliminate words below a frequency threshold, meaning that words that occur below the value will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987ee09-b2a3-4462-bf66-8a31cc5480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `no_below` parameter keeps tokens which are contained in at least `no_below` documents.\n",
    "# The `no_above` parameter represents the percent of tokens to keep which should be contained in no more than `no_above`*100% specified documents.\n",
    "    # This value is kept at 1.0 since the common words were already removed as part of the stop word removal process.\n",
    "    # So, remove words appearing in more than 100% of the documents (which is nothing, so no words are removed).\n",
    "# The `keep_n` parameter keeps the first `keep_n` most frequent tokens (or keep all if None) after applying the previous 2 parameters.\n",
    "dictionary.filter_extremes(no_below=10, no_above=1, keep_n=None)\n",
    "\n",
    "print(f\"Length of the dictionary after filtering: {len(dictionary):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116762f-944a-43c0-a01e-3144080f2cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert document into the bag-of-words format\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181c38a-796c-41e1-a130-83a82a76991a",
   "metadata": {},
   "source": [
    "Since LDA uses randomness within its algorithms, it yields slighty different output for different runs on the same data. \n",
    "To make sure that the output are consistent and to save some time, the model will be saved without having to rebuild it every single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa4f8b-6ccf-45e3-a164-2db7dafd4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ebc6f-54c7-436a-9231-a9787263f87c",
   "metadata": {},
   "source": [
    "<b>Note: </b> To make a new model, either keep the `make_new_model` variable's value to True or clear the contents in `model_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e073d2-3d3e-464a-8bdf-d1dde23880b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for storing all lda models\n",
    "model_dir = 'lda_checkpoint'\n",
    "if not os.path.isdir(model_dir):\n",
    "    print(f\"Making directory for LDA models: {model_dir}\")\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Make a new model if the `make_new_model` value is True\n",
    "# or no model file exists. \n",
    "model_path = os.path.join(model_dir, 'topic_model.lda')\n",
    "if make_new_model or not os.path.isfile(model_path):\n",
    "    print(f\"Since an LDA model doesn't already exist, a new model is being made and saved to \\\"{model_path}\\\"...\")\n",
    "\n",
    "    # Use parallelized Latent Dirichlet Allocation to parallelize and speed up model training\n",
    "    lda_model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 num_topics=10,\n",
    "                                                 per_word_topics=True)\n",
    "    # Save the model\n",
    "    lda_model.save(model_path)\n",
    "\n",
    "print('Loading model')\n",
    "lda_model = models.ldamodel.LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589bc59-4cae-4ba7-8d70-e7b01207d35b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interpreting LDA Topics\n",
    "\n",
    "* Upon constructing the model, our focus shifts to identifying topics acquired by the model, anticipating recognizable categories. \n",
    "* In LDA, a topic is a probability distribution over words, assigning distinct probabilities to each unique word in the dataset. \n",
    "* Examining the top 10 words for each topic reveals predominant themes. \n",
    "* Document-topic membership is expressed as a weight vector, where each weight signifies the document's representation of a specific topic.\n",
    "<br><b>Note: </b>LDA's distinctive feature is mixed membership, allowing documents to partially belong to multiple topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfaad71-748a-48fb-bff9-8b30f004c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 2 most significant topics.\n",
    "# Show in word-probability pairs.\n",
    "for topic in lda_model.show_topics(formatted=False)[:2]:\n",
    "    pprint(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858e211-e71d-439d-a3c6-17d7de8b4376",
   "metadata": {},
   "source": [
    "### Assessing Top Word Significance\n",
    "\n",
    "To delve deeper into topics, we can analyze the probability distribution (or weight) assigned to their top words. This involves visualizing the weights of the top 100 words, sorted by size, for each of the four topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd36cb3-042f-4483-a006-d2fb8d29f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# top 100 words by weight in each topic\n",
    "top_n_words = 100\n",
    "topics = lda_model.show_topics(num_topics=4,\n",
    "                               num_words=top_n_words,\n",
    "                               formatted=False)\n",
    "\n",
    "for _, infos in topics:\n",
    "    probs = [prob for _, prob in infos]\n",
    "    plt.plot(range(top_n_words), probs)\n",
    "\n",
    "# Reformat x-axis label and tick labels\n",
    "ax.set_xlabel('Word rank', fontsize=12, labelpad=10)  # No need for an axis label\n",
    "ax.xaxis.set_label_position(\"bottom\")\n",
    "ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=12, labelrotation=0)\n",
    "\n",
    "# Reformat y-axis\n",
    "ax.set_ylabel('Probability', fontsize=12, labelpad=10)\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "# Remove the spines\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "# Add in title\n",
    "ax.set_title(\"Probabilities of Top 100 Words in each Topic\", weight='bold')\n",
    "\n",
    "# Set a white background\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236293e-806c-456f-a6cc-8026cea28d7e",
   "metadata": {},
   "source": [
    "The plot illustrates a sharp decline in weights as we move down the list of most important words for each topic.\n",
    "<br>This pattern highlights that the first 10 to 20 words in each topic carry significantly greater weight than the subsequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a068ec-202c-42ef-bd0b-284ad944c76f",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca72b62-f1a7-4c17-afad-1585febdb628",
   "metadata": {},
   "source": [
    "Two evaluation metrics to consider for topic modeling: perplexity and coherence.\n",
    "* Perplexity: Captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. Similar to how well does the model represent or reproduce the statistics of the held-out data.\n",
    "* Coherence : A set of statements or facts is said to be coherent, if they support each other. Topic Coherence calculates the score of a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16489cc4-6d81-452e-bb60-df61ba3496ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity (lower is better)\n",
    "print(f\"Perplexity: {lda_model.log_perplexity(corpus):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caa65d-74b2-4888-b6e9-aa00b98b8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score (higher is better)\n",
    "coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                     texts=cleaned_sents, \n",
    "                                     dictionary=dictionary,\n",
    "                                     coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_lda:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823beab-fdc2-4a49-adcd-614ff4b246f7",
   "metadata": {},
   "source": [
    "Going forward, we will want to minimize perplexity and maximize coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40421737-8ac4-4376-9a2d-813bfecf080d",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning\n",
    "\n",
    "There are 3 main types of paramters that effect the LDA model:\n",
    "1. `num_topics`: Number of Topics\n",
    "2. `alpha`: Document-Topic Density\n",
    "3. `beta`: Word-Topic Density\n",
    "\n",
    "`alpha` is a parameter that controls the prior distribution over topic weights in each document, while `eta` is a parameter for the prior distribution over word weights in each topic. In gensim, both default to a `symmetric` (1 / `num_topics`).\n",
    "<br>`alpha` and `eta` can be thought of as smoothing parameters when we compute how much each document \"likes\" a topic (in the case of alpha) or how much each topic \"likes\" a word (in the case of gamma).\n",
    "<br><br>To find optimal values for these 3 parameters, use HyperOpt which is a Bayesian optimization library that employs probabilistic models to efficiently search through hyperparameter spaces, balancing exploration and exploitation. To find optimal values for the model, the code specifies the hyperparameter search space, defines an objective function, and lets HyperOpt iteratively evaluate and refine hyperparameters to maximize or minimize the chosen performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827d815-97e4-4b6f-81fb-768f6d7cbb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    num = int(params['num_topics'])\n",
    "    a = params['alpha']\n",
    "    b = params['eta']\n",
    "\n",
    "    # Use parallelized Latent Dirichlet Allocation to parallelize and speed up model training\n",
    "    lda_model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 num_topics=num,\n",
    "                                                 alpha=a,\n",
    "                                                 eta=b,\n",
    "                                                 per_word_topics=True)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         texts=cleaned_sents,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return -coherence_lda  # We use negative coherence_score because HyperOpt minimizes the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7156b-7655-4401-af57-99463f04ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'num_topics': hp.choice('num_topics', [2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "    'alpha': hp.choice('alpha', [0.001, 0.01, 0.1, 1.0, 'symmetric', 'asymmetric']),\n",
    "    'eta': hp.choice('eta', [0.001, 0.01, 0.1, 1.0, 'symmetric'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2530e86-52b6-42aa-83e3-dd26ebacff01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=500)\n",
    "\n",
    "best_params = {\n",
    "    'num_topics': [2, 3, 4, 5, 6, 7, 8, 9, 10][best['num_topics']],\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 'symmetric', 'asymmetric'][best['alpha']],\n",
    "    'eta': [0.001, 0.01, 0.1, 1, 'symmetric'][best['eta']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f5d50-08ce-4df4-8979-9749c0a5e4d5",
   "metadata": {},
   "source": [
    "Best Hyperparamters (as selected by HyperOpt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9765d-01ff-48bd-8a58-5fa16d45df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e94d5-c13a-4734-80da-9317e6121b81",
   "metadata": {},
   "source": [
    "Build the final model using the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818ef9c-ec31-4df4-ba39-c92b8e36c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parallelized Latent Dirichlet Allocation to parallelize and speed up model training\n",
    "lda_model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                             id2word=dictionary,\n",
    "                                             num_topics=best_params['num_topics'],\n",
    "                                             alpha=best_params['alpha'],\n",
    "                                             eta=best_params['eta'],\n",
    "                                             per_word_topics=True)\n",
    "# Save the model\n",
    "lda_model.save(model_path)\n",
    "\n",
    "# Load the model\n",
    "lda_model = models.ldamodel.LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04632eb-9b21-4e60-9149-11d0f027db8b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949ef67-2593-459e-8677-473de6cbea6d",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Using `pyLDAvis.gensim.prepare`, the code below generates an interactive visualization for the LDA model above.\n",
    "\n",
    "* **Interactive Topic Visualization:**\n",
    "   - The main area displays a scatter plot of topics, where each bubble represents a topic.\n",
    "   - The size of each bubble indicates the prevalence of the topic in the entire corpus.\n",
    "   - Topics that are closer on the plot are more similar in terms of the words they contain.\n",
    "\n",
    "* **Topic-Term Distribution:**\n",
    "   - On the right side, there is a bar chart showing the top terms for the selected topic.\n",
    "   - The length of the bars represents the frequency of each term within the selected topic.\n",
    "\n",
    "* **Relevance:**\n",
    "   - The relevance of each term in the topic is indicated by the color of the bars in the right panel.\n",
    "   - Darker colors represent higher relevance, helping to identify key terms within each topic.\n",
    "\n",
    "**Note:** The visualization allows for interactive exploration. Hovering over a bubble or a term displays additional information. Selecting a term updates the right panel with information about its distribution across topics. The code below saves the visualization as an html file, `LDA_vis.html`, which can be opened in a browser and explored further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bd64f-3b92-4037-9d64-74644d0ab7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4a910-94ea-40e5-9396-41281516bcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976c90c-8b66-4f09-9658-6fcd2e820e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p, 'LDA_vis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19e8cb-fb15-4b4b-bd20-54b99e0f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run the visualization in this notebook\n",
    "# # p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
