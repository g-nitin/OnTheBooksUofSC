{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2409e4ce-2e4f-4029-ae7c-842dfa3294a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentence Splitting\n",
    "This notebook uses OCRed text for a volume year and splits it into sentences using regular expression pattern matching.<br>\n",
    "For this notebook to run, there should be an OCRed folder that should contain a .txt file, a .tsv file, and an images sub-folder (more details in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261971e-eaad-4191-a8f9-279c31471815",
   "metadata": {},
   "source": [
    "<b>Note:</b>\n",
    "- If the Acts and Joints were mixed for the chosen year, the OCRed output will contain `{year}_Both.txt` and `{year}_Both_data.tsv`\n",
    "- If the Acts and Joints were seperate for the chosen year, the OCRed output will contain `{year}_Acts.txt` and `{year}_Acts_data.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce678b-a95e-48af-b86f-f96be48a238c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T21:10:35.381355Z",
     "start_time": "2023-07-12T21:10:35.372055Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66ec11-f426-4dae-94d3-a834d27f8dfa",
   "metadata": {},
   "source": [
    "<br>\n",
    "Either get the year variable from elsewhere (such as when this notebook is accessed from another file) or specify the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c488195-d210-4c2e-854f-5936a66dc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the year variable from somewhere else\n",
    "%store -r year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b101c-1fbb-4a41-a3b7-5ce88b10bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running this notebook independently,\n",
    "# # Uncoment the following line of code\n",
    "# year = 1892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4feccc3-04a2-4689-9b18-6052d27df568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory that will contain the OCRed output:\n",
    "dir_OCR = \"/Users/nitingupta/Desktop/OTB/OCRed/\" + str(year)\n",
    "\n",
    "print(f\"Working on {year} under {dir_OCR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b1c4b-ad16-4478-90ac-7be1b3b8aa99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try reading in \"{year}_text.txt\" if the Acts and Joints were seperate for the year\n",
    "try:\n",
    "    acts_path = dir_OCR + \"/\" + str(year) + \"_Acts.txt\"\n",
    "    with open(acts_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # If the read is successful, set a flag that identifies that the Acts and Joints are seperate\n",
    "    actsSep = True\n",
    "\n",
    "# However, if the directory contains {year}_Both.txt instead, a FileNotFoundError will be returned for the above code.\n",
    "# So, catch that error and read in \"{year}_Both.txt\"\n",
    "except FileNotFoundError:\n",
    "    acts_path = dir_OCR + \"/\" + str(year) + \"_Both.txt\"\n",
    "    with open(acts_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    actsSep = False  # The flag being False means that the Acts and Joints are not seperate\n",
    "\n",
    "# This variable holds all the OCRed text as a String\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9839aca-b497-4efe-9d19-855a2df045d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of pages OCRed for {year} is: {count}\".format(year = year, count = (data.count(\"\\n\\n\")+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe794dde-3fe2-4f02-93f8-3d6098e0866b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46f133-3523-4079-9bbf-b19400f1c3cf",
   "metadata": {},
   "source": [
    "## A. Training the tokenizer\n",
    "Based on this [article](https://subscription.packtpub.com/book/application-development/9781782167853/1/ch01lvl1sec12/training-a-sentence-tokenizer),\n",
    "- NLTK's default sentence tokenizer is general purpose and usually works quite well. But sometimes it might not be the best choice for our text if it uses nonstandard punctuation or is formatted in a unique way. In such cases, training your own sentence tokenizer can result in much more accurate sentence tokenization.\n",
    "- The `PunktSentenceTokenizer` class uses an unsupervised learning algorithm to learn what constitutes a sentence break.\n",
    "    - The specific technique used in this case is called sentence boundary detection. It works by counting punctuation and tokens that commonly end a sentence, such as a period or a newline, then using the resulting frequencies to decide the sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e787f-ba8d-4f47-a8e0-5b2d3b7cc708",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer(data)\n",
    "sentences = sent_tokenizer.tokenize(data)\n",
    "\n",
    "# A List of tokens/sentences as seperated by nltk's PunktSentenceTokenizer\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5697fa-8a7b-4575-9a70-e334f4e4f175",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a408d-8d8e-4dd9-8d3d-1ad62e1dead1",
   "metadata": {},
   "source": [
    "## B. Creating the dataframe\n",
    "Make a new dataframe with the sentences and character lengths as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c17c5-5ad3-4371-8380-6728bee2863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to a new DataFrame\n",
    "df = pd.DataFrame()\n",
    "df[\"sentence\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e4176-aa1b-42b2-af4b-2bc2167be841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = [len(sentence) for sentence in sentences]\n",
    "print(\"Length of the initial dataframe:\", df.shape[0], \"\\nThis is the number of tokenized sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c4b9c-b677-46c0-aa2c-4edb32488386",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb28216-d4fb-4409-bfbf-403d1c3e2727",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Adding page file names\n",
    "- Add an feature that specifies which page number that sentence starts and ends on.\n",
    "- Reading only Acts. <b> Not reading Joints </b>\n",
    "- The reason to read the files from the directory is to ensure that missing file pages are not missed in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a008ea8-445d-48f7-9de4-e3302b38db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path to the directory that contains the images.\n",
    "# NOTE: This directory is inside the OCRed output for the chosen year\n",
    "dir_imgs = dir_OCR + \"/images\"\n",
    "print(f\"The images directory is {dir_imgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716cafc-aefb-4920-ac01-e5cc88c4250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = os.listdir(dir_imgs)\n",
    "imgs = [img for img in imgs if \"jpg\" in img or \"tiff\" in img or \"JPG\" in img or \"TIFF\" in img]\n",
    "imgs.sort()\n",
    "print(\"The number of image files for this year is:\", len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f7dc7-9d14-43db-af56-c1c644b1b882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileType = imgs[0].split(\".\")[1]\n",
    "print(f\"The files are of type: {fileType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca102b90-a98d-4407-9e9a-85d234537da5",
   "metadata": {},
   "source": [
    "<b>Note:</b>\n",
    "- The OCR attempts to seperates new pages by adding \"\\n\\n\". However, the total number of pages does not equal the total count of \"\\n\\n\" in the text as the OCR does not add \"\\n\\n\" after every page.\n",
    "- One way to eliminate this issue is by utilizing the `{year}_Both_data.tsv` (if acts and joints mixed) or `{year}_Acts_data.tsv` (if acts and joints seperated) file from the OCR output.\n",
    "- This file contains each word (in the 2nd last column) and the filename for that word (last column).\n",
    "- Also, since we are only working with Acts, if the Acts and Joints are seperate, the last word in the df_word dataframe will not end on the actual last page in the images sub-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b0292-77f3-40c7-9df5-fc5572944b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on whether the Acts and Joints are mixed, read the appropriate tsv file\n",
    "if actsSep:\n",
    "    df_words = pd.read_table(f\"{dir_OCR}/{year}_Acts_data.tsv\")\n",
    "else:\n",
    "    df_words = pd.read_table(f\"{dir_OCR}/{year}_Both_data.tsv\")\n",
    "\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deaebfe-30b8-4078-9543-cf10059129ac",
   "metadata": {},
   "source": [
    "So, to label the page numbers in the dataframe, we can go through the original dataframe and find the start and end words in each sentence.\n",
    "<br>We, can then find the page numbers for those words, from `df_words` and add them to the original dataframe, `df`.\n",
    "<br>To start, we need to clean the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5474d57-cba4-4a4b-8f69-832dbcdeb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page'] = np.nan\n",
    "\n",
    "# Drop the columns which are unessecary for our analysis\n",
    "df_words.drop(columns=[\"left\", \"top\", \"width\", \"height\", \"conf\"], inplace=True)\n",
    "\n",
    "# Drop the rows which don't contain a word in the \"text\" column\n",
    "df_words.dropna(inplace=True)\n",
    "\n",
    "# Relabel the \"name\" column to \"page\" column\n",
    "df_words.rename(columns={\"name\": \"page\"}, inplace=True)\n",
    "\n",
    "# Reassign index after dropping nas\n",
    "df_words = df_words.assign(row_number=range(len(df_words)))\n",
    "df_words.set_index('row_number', inplace=True)\n",
    "\n",
    "# Drop the 'page' column from the org dataframe\n",
    "df.drop(columns=['page'], inplace=True)\n",
    "\n",
    "# Add an empty 'start_page' and 'end_page' column\n",
    "df['start_page'] = np.nan\n",
    "df['end_page'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fba37f-602b-4256-9d0e-e58ed172d65d",
   "metadata": {},
   "source": [
    "Since, a word can only exist on a single page, we have unique identifiers for the start and end page for each sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f2d2a-938f-4e58-b055-a15eb8e65cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tracker for df_words:\n",
    "words_trkr = 0\n",
    "\n",
    "# Loop over the original dataframe\n",
    "for i in range(0, df.shape[0]):\n",
    "    \n",
    "    # Remove \"\\n\\n\" from the original dataframe as they will interfere with the analysis\n",
    "    df.at[i, 'sentence'] = df.iloc[i]['sentence'].replace(\"\\n\\n\", \"\")\n",
    "\n",
    "    # For each sentence, extract the first and last word\n",
    "    tmp_sentence = df.iloc[i]['sentence'].split(\" \")\n",
    "    start, last = tmp_sentence[0], tmp_sentence[-1]\n",
    "\n",
    "    # Get the page number for the start and end word\n",
    "    start_page = df_words.iloc[words_trkr]['page']\n",
    "\n",
    "    try:\n",
    "        end_page = df_words.iloc[words_trkr + len(tmp_sentence)]['page']\n",
    "    except:\n",
    "        end_page = df_words.iloc[words_trkr]['page']\n",
    "        \n",
    "\n",
    "    # Remove the filename from the pages:\n",
    "    start_page = start_page.split(\".\")[0]\n",
    "    end_page = end_page.split(\".\")[0]\n",
    "\n",
    "    \n",
    "    # Assign the page number to their respective columns in the dataframe\n",
    "    df.at[i, 'start_page'] = start_page\n",
    "    df.at[i, 'end_page'] = end_page\n",
    "    \n",
    "    # Update tracker\n",
    "    words_trkr += len(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e208f-775a-4233-9c3f-a8bc376f96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d3256-7f66-42ab-b5f9-263c1f40f6b3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af059985-bf5d-41f9-b15c-bc0613b85405",
   "metadata": {
    "tags": []
   },
   "source": [
    "## D. Cleaning on Char. lenght\n",
    "Get rid of sentences with a low number of characters as they might not form meaningful sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb162e6-5498-49ca-ba81-690fbb89d68f",
   "metadata": {},
   "source": [
    "However, first, get the statistics on the length column to avoid removing meaningful sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adad633-bd47-4d6a-94fb-dd773211a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the statistics for the length column\n",
    "df[\"length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa067b-6286-47b3-aa2a-7929ae9676d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram for that column\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "df.hist(column=\"length\", ax=axes[0])\n",
    "axes[0].set_title('Character lengths')\n",
    "axes[0].set(xlabel=\"Character length of the sentences\", ylabel=\"Frequency\")\n",
    "\n",
    "df.hist(column=\"length\", ax=axes[1])\n",
    "\n",
    "axes[1].set_title('Character lengths (zoomed)')\n",
    "axes[1].set(xlabel=\"Character length of the sentences\", ylabel=\"Frequency\", xlim = [0, 1000])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a6795-6d0b-4757-9d56-b586eb9b7412",
   "metadata": {},
   "source": [
    "Define a cutoff for the sentences. All sentences belows this length will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ac75b-a0ab-4f3c-8bec-29d30ab37422",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78172d2-3e9a-4889-a842-429273fa05b0",
   "metadata": {},
   "source": [
    "Create a smaller dataframe, and export it to csv, that only contains the short length sentences.\n",
    "Check the csv and change the length condition accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df695759-1e0d-4d5c-abfc-b81b5d4f02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment the following line of code to create the csv which contains the short length sentences.\n",
    "\n",
    "# testing_df = df[df['length'] < cut_len]\n",
    "# testing_df.to_csv(f\"{year}_len_testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bdce2-9ba6-430d-99e3-6ebf3c92705f",
   "metadata": {},
   "source": [
    "<br>\n",
    "Once, the length is decided, create a new dataframe with sentences greater that the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db1f3f-7454-4e23-819f-698c536bb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df[ df[\"length\"] > cut_len]\n",
    "print(\"Length of the cleaned dataframe: \", df_reduced.shape[0])\n",
    "print(\"Reduction of about {:.2f}%\".format( (1 - df_reduced.shape[0]/df.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51144099-ae02-4949-849c-48e5b271ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.reset_index(drop=True, inplace=True)\n",
    "df_reduced.index.name = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77376f41-ff13-4fc3-80f1-096880ede75f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639c194-db50-4444-ba6f-a9eb376ad89a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## E. Regex Matching\n",
    "Remove unecessary words, which do not contribute to the overall meaning, in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae0635-75aa-44af-afa7-82872093d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe so that the results of the matching can be compared\n",
    "df_cleaned = df_reduced.copy()\n",
    "\n",
    "# A new dictionary to keep track of the number of errors\n",
    "errorsDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18420305-ce14-4db6-a326-81fd1a0771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that will contain the removed words that match the section patter\n",
    "df_cleaned['removed'] = np.nan\n",
    "\n",
    "# Rename 'sentence' column to 'org_sent' to avoid confusion\n",
    "df_cleaned.rename(columns={'sentence': 'org_sent'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3490cba-5742-4004-8b1f-c211f59da561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e81d6-95e3-4602-aa69-cf4c23469fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceInDF(rgx_match: re.Pattern, df: pd.DataFrame, prevAppend: bool):\n",
    "    '''\n",
    "    Find the provided regex pattern in the provided dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rgx_match : re.Pattern\n",
    "        A regular expression pattern that will be search for and replaced in the df\n",
    "    df: pandas.Dataframe\n",
    "        A Pandas dataframe to search and replace for\n",
    "        Should contain an:\n",
    "            'org_sent' column, in which the matches which will be replaced\n",
    "             'removed' column, in which the matched string will be stored\n",
    "     prevAppend: bool\n",
    "         A flag for whether the match should be append to the end of the previous sentence\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple consisting of:\n",
    "    \n",
    "    df: pandas.Dataframe\n",
    "        The modified Dataframe with the matches performed\n",
    "    errorCount: int\n",
    "        A count of how many times this error was found.\n",
    "    '''\n",
    "    \n",
    "    errorCount = 0\n",
    "    \n",
    "    for i in range(0, df.shape[0]):\n",
    "    \n",
    "        # The value at this row's \"removed\" column\n",
    "        removed_val = df.iloc[i]['removed']\n",
    "        \n",
    "        # The found matches\n",
    "        matches = \"; \".join([x.group() for x in re.finditer(rgx_match, df.iloc[i]['org_sent'])])\n",
    "        \n",
    "        # if no match found...\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        # Else if match is found...\n",
    "        \n",
    "        # Update the counter for the error with the number of matches found\n",
    "        errorCount += len(matches.split(\";\"))\n",
    "        \n",
    "        # Check if there is already a value in the 'removed' column for that row\n",
    "        if removed_val != \"\" and not pd.isnull(removed_val):\n",
    "            # Append the matches to the existing value seperated by \";\"\n",
    "            df.at[i, 'removed'] = str(removed_val) + \"; \" + matches\n",
    "        else:\n",
    "            # Add the matched patterns to the \"removed\" section seperated by \";\"\n",
    "            df.at[i, 'removed'] = matches\n",
    "        \n",
    "        if prevAppend and i != 0:\n",
    "            \n",
    "            m = re.search(rgx_match, df.iloc[i]['org_sent'])\n",
    "            if m:\n",
    "                # Add to the end of the previous sentence\n",
    "                df.at[i-1, 'org_sent'] = df.iloc[i-1]['org_sent'] + \" \" + str(m.group())\n",
    "            \n",
    "        # Remove the matched patterns from sentences\n",
    "        df.at[i, 'org_sent'] = re.sub(rgx_match, '', df.iloc[i]['org_sent'])    \n",
    "        \n",
    "    return df, errorCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e332003-850b-427a-bb4a-d3e533374356",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102c656-de20-4078-950c-eb0140134fe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Removing section identifiers\n",
    "The following code implements regex patterns to identify sections, such as \"Section 1.\", \"Sec. 4.\", etc. \n",
    "<br>Since most sections, which need to be removed, appear either at the start or the start of the ORCed sentence, the pattern finds matches either at the start or the end of the sentence.\n",
    "<br>Do note that the same pattern is repeated for the start and end of the sentence, and is seperated by '|'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72579-6be3-4aef-86ec-88e8d3bc2a4d",
   "metadata": {},
   "source": [
    "Some notes about the pattern:\n",
    "- `r'(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}'` matches \"Section\"\n",
    "- `r'(\\.|,|:|;| )'{0,2}` matches mistaken delimiters or spaces following \"Section\"\n",
    "- `r'[0Oo1Iil!2Z5S6G\\d]{1,2}'` matches the section number. Letters are required in this pattern to account for OCR mistakes\n",
    "- `r'(. |.| |)'` matches the end of phrase spaces and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6e603-2728-4141-a0dd-c9f5781d2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile(r\"^(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)|(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)$\")\n",
    "\n",
    "df_cleaned, errorsDict['section identifiers'] = replaceInDF(rgx_match, df_cleaned, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b1946-60c1-456e-98f0-0d5fc9b66ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17567a4e-23d7-4c7f-9436-275ba5c01555",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da10614-bbb0-417d-b642-f82a0ff77b0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Removing end of line hyphenation\n",
    "Whenever a word in the sentence continues from the end of a line to the beginning of the next line and is joined by a hyphen, the OCRed sentence also contains that hyphen and a space.\n",
    "<br>For example, 'Commander-in-Chief' is OCRed as 'Com- mander-in-Chief'\n",
    "<br>The following code implements regex patterns to remove \"- \" in the text since each hyphenated word is split with \"- \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e8336-9cd0-436e-9968-eadd7ec8604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile('[-][ ]')\n",
    "df_cleaned, errorsDict['EOL hyphenation'] = replaceInDF(rgx_match, df_cleaned, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb90e9-0cbd-49e7-835a-fbb1c2638513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d325d-db8c-489b-9581-8986a2da64b9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1064a-5502-4371-9f3d-82e1fca06965",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Relocating incorrect \"Approved ...\" phrases\n",
    "The “Approved…” phrases are incorrectly appended to the start of the next law. They should by appended to the end of the previous law.\n",
    "<br>Phrases might be of the format: \n",
    "- \"Approved the 2oth day of February, A. D. 1901\",\n",
    "- \"Approved December 15th, A. D. 1892.\",\n",
    "- \"Approved December O5th, A. D. 1892.\",\n",
    "- \"Approved December !2th, A. D. 1892.\",\n",
    "- \"Approved December 6Gth, A. D. 1892.\",\n",
    "- \"Approved December 05th, A. D. 1892.\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc0ac7-7db3-41e4-a1dd-f900848bf1d1",
   "metadata": {},
   "source": [
    "Since phrases might either have the month or the date after the \"Approved\" sub-string, the code below utilized two patterns to account for either case, seperated by '|'.\n",
    "<br>\n",
    "Some notes about the pattern:\n",
    "- `r'[0Oo1Iil!2Z5S6G\\d]{1,2}'` matches the date. Letters are required in this pattern to account for OCR mistakes\n",
    "- `r'(?:th|st|nd|rd)'` matches the prefixes for the dates\n",
    "- `r'[A-Z][a-z]+'` matches the month\n",
    "- `r'(?:18\\d{2}|19\\d{2}|2\\d{3})'` matches years ranging from 18** to 2***\n",
    "- `r'(. |.| |)'` matches the end of phrase spaces and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba4fd1-6857-4217-809f-8f277e077352",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile(\n",
    "    r'^Approved the [0Oo1Iil!2Z5S6G\\d]{1,2}(?:th|st|nd|rd) day of [A-Z][a-z]+, A\\. D\\. (?:18\\d{2}|19\\d{2}|2\\d{3})(. |.| |)\\b|Approved [A-Z][a-z]+ [0Oo1Iil!2Z5S6G\\d]{1,2}(?:th|st|nd|rd), A\\. D\\. (?:18\\d{2}|19\\d{2}|2\\d{3})(. |.| |)\\b')\n",
    "df_cleaned, errorsDict['Approved phrases'] = replaceInDF(rgx_match, df_cleaned, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef324fad-9049-4929-849f-381bd8f74aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0df6c7-380a-497c-81a1-216a9bf2b824",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2da0f-3d43-4b4e-b8f9-e64061ab9b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Removing Act seperators\n",
    "The horizontal lines differentiating one Act from another show up as U+2014 : EM DASH characters (one or multiple) in the OCR.\n",
    "<br>For example, '——- —— AN ACT...' or '—— AN ACT...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b379ea0-57e1-4435-b26d-49b7eb7768f4",
   "metadata": {},
   "source": [
    "Some notes about the pattern:\n",
    "- `r'^—+'` matches one or more consecutive occurrences of the \"—\" character at the start of a line.\n",
    "- `r'(?=\\s*[A-Za-z])'` is a positive lookahead (this part isn't captured) that checks if there is zero or more whitespace characters (\\s*) followed by a letter ([A-Za-z]) after the \"—\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08aec03-32f2-441a-ad50-dbe04fde5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile(r'^—+(?=\\s*[A-Za-z])')\n",
    "df_cleaned, errorsDict['Act seperators'] = replaceInDF(rgx_match, df_cleaned, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab2d8a-d0ba-4201-913f-86ee7df1a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41cb49-38f2-4dbf-b32d-3fdb51824926",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d59afb-2023-4127-878d-5b03c9d5b1c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Removing incorrect numbers at the start\n",
    "Some numbers are incorrectly left at the start of the sentence from the OCR process. They are rather OCRed, for example, as 2, or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88befb-cf7f-48b4-ac02-4560ab50c499",
   "metadata": {},
   "source": [
    "Some notes about the pattern:\n",
    "- `r'^[0Oo1Iil!2Z5S6G\\d]{1,3}'` matches upto 3 numbers at the start of the string\n",
    "- `r'(. |.| |)'` matches the end of phrase spaces and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552e645-4c7e-45c5-879e-f58096655fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile(r'^[0Oo1Iil!2Z5S6G\\d]{1,3}(. |.| |)')\n",
    "df_cleaned, errorsDict['Incorrect starting nums'] = replaceInDF(rgx_match, df_cleaned, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458706f-6a43-43b1-9e8e-3e0c2199dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756e44c-58ed-4e7d-84ac-06bd48853bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errorsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562628b-e9b8-45de-96a4-e173b34f1d0b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0294a-e3cf-47b2-919d-a49330d36996",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F. Adding features\n",
    "The features added below are:\n",
    "- an id: a concatenation of the year and index number\n",
    "- whether the sentence is an Act or a Joint\n",
    "- the state that the law originates from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d5896-09bc-4db1-86ae-8f583d16431e",
   "metadata": {},
   "source": [
    "### 1. Adding ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ed736-fffa-4c51-863a-ad181411e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPrefix(fileName: str, nameLen: int, fileType: bool) -> str:\n",
    "    '''\n",
    "    Since the fileNames from the excel parsing could be any of any length\n",
    "    (ranging from 1-3), this function appends a string of 0's to the \n",
    "    start of the input so that it is the specified nameLen lengths long.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fileName : str\n",
    "        The file name that needs to be prefixed\n",
    "    nameLen : int\n",
    "        The length of the expected name of the file\n",
    "        Ex. '00034.jpg' would have length of 5\n",
    "        so nameLen should be 5\n",
    "    fileType: bool\n",
    "        True if the fileName contains a fileType prefix such as '.tiff'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A length 5 file name (prefixed with 0's)\n",
    "    '''\n",
    "    \n",
    "    # Remove the file type\n",
    "    if fileType:\n",
    "        name = fileName.split(\".\")[0]\n",
    "    else:\n",
    "        name = fileName\n",
    "\n",
    "    prefix_length = nameLen - len(name)\n",
    "    prefix = \"0\" * prefix_length\n",
    "    \n",
    "    final_string = prefix + fileName\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801592f-d1d4-4908-b565-3d23737fca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.reset_index(inplace=True)\n",
    "df_cleaned.rename(columns={\"index\" : \"id\"}, inplace=True)\n",
    "# df_cleaned.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ca367-11fa-4901-9562-84450319fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the id of the last row in the dataframe, which is used to assess how many 0's will be prefixed to the other ids\n",
    "maxNumLength = len(str(df_cleaned.last_valid_index()))\n",
    "\n",
    "for i in range(0, df_cleaned.shape[0]):\n",
    "    df_cleaned.at[i, 'id'] = str(year) + \"_\" + addPrefix(str(df_cleaned.iloc[i]['id']), maxNumLength, fileType=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e267c36-e5bb-472c-8ffa-0e92b75fe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80060194-c068-4235-9b6c-cae29806afd0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4cdda-7013-42b3-9a9b-166f0c5ccc0b",
   "metadata": {},
   "source": [
    "### 2. Adding the remaining identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb6961-6e94-400c-b5b6-5a9621a68af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.insert(1, 'law_type', 'Acts')\n",
    "df_cleaned.insert(2, 'state', 'SOUTH CAROLINA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae9f68-18b5-4af2-bea1-7cf8f2598c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ec8e1-d79e-4467-86e1-4fb1293d8360",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b23a33-3387-4c9e-878e-3d3bf6e218dd",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af98825-e875-4183-b848-896a41d999c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'removed' column\n",
    "df_cleaned.drop(['removed'], axis = 1, inplace=True)\n",
    "\n",
    "# Rename the 'org_sent' column\n",
    "df_cleaned.rename(columns={\"org_sent\": \"sentence\"}, inplace=True)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcac79e-856a-449e-bffb-c80e72d9fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final dataframe to csv for viewing\n",
    "# df_cleaned.to_csv(f\"{year}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
