{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Faster Sentence Splitting\n",
    "This notebook uses OCRed text for a volume year and splits it into sentences using regular expression pattern matching.<br>\n",
    "For this notebook to run, there should be an OCRed folder that should contain a .txt file, a .tsv file, and an images sub-folder (more details in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the code, please view `sentence_splitting_explained.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T21:10:35.381355Z",
     "start_time": "2023-07-12T21:10:35.372055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Either get the year variable from elsewhere (such as when this notebook is accessed from another file) or specify the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the year variable from somewhere else\n",
    "%store -r year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running this notebook independently,\n",
    "# # Uncoment the following line of code\n",
    "# year = '1901'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1901 under /work/otb-lab/OCRed/1901\n"
     ]
    }
   ],
   "source": [
    "# This is the directory that will contain the OCRed output:\n",
    "dir_OCR = \"/work/otb-lab/OCRed/\" + str(year)\n",
    "\n",
    "print(f\"Working on {year} under {dir_OCR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'acts_path' in globals():\n",
    "    print(\"Deleting acts_path\")\n",
    "    del acts_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/otb-lab/OCRed/1901/1901_Acts.txt\n"
     ]
    }
   ],
   "source": [
    "# Try reading in \"{year}_text.txt\" if the Acts and Joints were seperate for the year\n",
    "try:\n",
    "    # Try to get a filename in the directory which has 'act' and 'text' in it\n",
    "    for file in listdir(dir_OCR):\n",
    "        \n",
    "        if \"txt\" in file.lower() and \"joint\" not in file.lower() and \"concurrent\" not in file.lower() and \"bills\" not in file.lower():\n",
    "            if \"acts\" in file.lower():\n",
    "                acts_path = dir_OCR + \"/\" + file\n",
    "                break\n",
    "                \n",
    "            elif \"act\" in file.lower():\n",
    "                acts_path = dir_OCR + \"/\" + file\n",
    "                break\n",
    "    \n",
    "    print(acts_path)\n",
    "    with open(acts_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # If the read is successful, set a flag that identifies that the Acts and Joints are seperate\n",
    "    actsSep = True\n",
    "\n",
    "# However, if the directory contains {year}_Both.txt instead, a FileNotFoundError will be returned for the above code.\n",
    "# So, catch that error and read in \"{year}_Both.txt\"\n",
    "except:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "    \n",
    "        # Try to get a filename in the directory which has 'both' and 'text' in it\n",
    "        for file in listdir(dir_OCR):\n",
    "\n",
    "            if \"both\" in file.lower() and \"txt\" in file.lower() and \"joint\" not in file.lower() and \"concurrent\" not in file.lower() and \"bills\" not in file.lower():\n",
    "                acts_path = dir_OCR + \"/\" + file\n",
    "\n",
    "        print(acts_path)\n",
    "        with open(acts_path, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        actsSep = False  # The flag being False means that the Acts and Joints are not seperate\n",
    "        \n",
    "        \n",
    "    # However, some years might contain {year}_Acts_Joints.txt\n",
    "    except:\n",
    "        \n",
    "        # Try to get a filename in the directory which has 'both' and 'text' in it\n",
    "        for file in listdir(dir_OCR):\n",
    "\n",
    "            if \"acts\" in file.lower() and \"joints\" in file.lower() and \"txt\" in file.lower():\n",
    "                acts_path = dir_OCR + \"/\" + file\n",
    "\n",
    "        print(acts_path)\n",
    "        with open(acts_path, 'r') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        actsSep = False  # The flag being False means that the Acts and Joints are not seperate\n",
    "\n",
    "\n",
    "# This variable holds all the OCRed text as a String\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pages OCRed for 1901 is: 258\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of pages OCRed for {year} is: {count}\".format(year = year, count = (data.count(\"\\n\\n\")+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Training the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer(data)\n",
    "sentences = sent_tokenizer.tokenize(data)\n",
    "\n",
    "# A List of tokens/sentences as seperated by nltk's PunktSentenceTokenizer\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to a new DataFrame\n",
    "df = pd.DataFrame()\n",
    "df[\"sentence\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Strip sentences of trailing and leading whitespaces\n",
    "df['sentence'] = df['sentence'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the initial dataframe: 1864 \n",
      "This is the number of tokenized sentences.\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the initial dataframe:\", df.shape[0], \"\\nThis is the number of tokenized sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Adding page file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images directory is /work/otb-lab/OCRed/1901/images\n",
      "The number of image files for this year is: 270\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dir_imgs = dir_OCR + \"/images\"\n",
    "    imgs = listdir(dir_imgs)\n",
    "except FileNotFoundError:\n",
    "\n",
    "    try:\n",
    "        dir_imgs = dir_OCR + \"/Images\"\n",
    "        imgs = listdir(dir_imgs)\n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        try:\n",
    "            dir_imgs = dir_OCR + \"/images.zip\"\n",
    "            imgs = listdir(dir_imgs)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            \n",
    "            try:\n",
    "                dir_imgs = dir_OCR + \"/Images.zip\"\n",
    "                imgs = listdir(dir_imgs)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                dir_imgs = dir_OCR + \"/\" + year\n",
    "                imgs = listdir(dir_imgs)\n",
    "            \n",
    "\n",
    "print(f\"The images directory is {dir_imgs}\")\n",
    "    \n",
    "imgs = [img for img in imgs if \"jpg\" in img or \"tiff\" in img or \"JPG\" in img or \"TIFF\" in img]\n",
    "imgs.sort()\n",
    "print(\"The number of image files for this year is:\", len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files are of type: jpg\n"
     ]
    }
   ],
   "source": [
    "fileType = imgs[0].split(\".\")[1]\n",
    "print(f\"The files are of type: {fileType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/otb-lab/OCRed/1901/1901_Acts_data.tsv\n"
     ]
    }
   ],
   "source": [
    "for file in listdir(dir_OCR):\n",
    "    if \"tsv\" in file.lower():\n",
    "        \n",
    "        if actsSep:           \n",
    "            \n",
    "            # Try to get a filename in the directory which has 'act' (or 'acts') and 'tsv' in it\n",
    "            if \"act\" in file.lower() or \"acts\" in file.lower():\n",
    "                words_path = dir_OCR + '/' + file\n",
    "               \n",
    "        else:\n",
    "    \n",
    "            if \"both\" in file.lower() or \"joints\" in file.lower():\n",
    "                words_path = dir_OCR + '/' + file               \n",
    "\n",
    "\n",
    "df_words = pd.read_table(words_path)\n",
    "print(words_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page'] = pd.NA\n",
    "\n",
    "# Drop the columns which are unessecary for our analysis\n",
    "df_words.drop(columns=[\"left\", \"top\", \"width\", \"height\", \"conf\"], inplace=True)\n",
    "\n",
    "# Drop the rows which don't contain a word in the \"text\" column\n",
    "df_words.dropna(inplace=True)\n",
    "\n",
    "# Relabel the \"name\" column to \"page\" column\n",
    "df_words.rename(columns={\"name\": \"page\"}, inplace=True)\n",
    "\n",
    "# Reassign index after dropping nas\n",
    "df_words = df_words.assign(row_number=range(len(df_words)))\n",
    "df_words.set_index('row_number', inplace=True)\n",
    "\n",
    "# Drop the 'page' column from the org dataframe\n",
    "df.drop(columns=['page'], inplace=True)\n",
    "\n",
    "# Add an empty 'start_page' and 'end_page' column\n",
    "df['start_page'] = pd.NA\n",
    "df['end_page'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"\\n\\n\" from the original dataframe as they will interfere with the analysis\n",
    "df['sentence'] = df['sentence'].str.replace(\"\\n\\n\", \"\", regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tracker for df_words:\n",
    "words_trkr = 0\n",
    "\n",
    "# Loop over the original dataframe\n",
    "for i in range(0, df.shape[0]):\n",
    "\n",
    "    # For each sentence, extract the first and last word\n",
    "    tmp_sentence = df.iloc[i]['sentence'].split(\" \")\n",
    "    start, last = tmp_sentence[0], tmp_sentence[-1]\n",
    "    \n",
    "    # Get the page number for the start and end word\n",
    "    try:\n",
    "        start_page = df_words.iloc[words_trkr]['page']\n",
    "    except IndexError:\n",
    "        try:\n",
    "            words_trkr -= len(tmp_sentence)\n",
    "            start_page = df_words.iloc[words_trkr]['page']\n",
    "        except:\n",
    "            start_page = df_words['page'].iloc[-1]\n",
    "    \n",
    "    try:\n",
    "        end_page = df_words.iloc[words_trkr + len(tmp_sentence)]['page']\n",
    "    except IndexError:\n",
    "        try:\n",
    "            end_page = df_words.iloc[words_trkr]['page']\n",
    "        except:\n",
    "            end_page = df_words['page'].iloc[-1]\n",
    "    \n",
    "\n",
    "    # Remove the filename from the pages:\n",
    "    start_page = start_page.split(\".\")[0]\n",
    "    end_page = end_page.split(\".\")[0]\n",
    "\n",
    "    \n",
    "    # Assign the page number to their respective columns in the dataframe\n",
    "    df.at[i, 'start_page'] = start_page\n",
    "    df.at[i, 'end_page'] = end_page\n",
    "    \n",
    "    # Update tracker\n",
    "    words_trkr += len(tmp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## D. Further Cleaning and Regex\n",
    "Remove unecessary words in the sentences which do not contribute to the overall meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe so that the results of the matching can be compared\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# A new dictionary to keep track of the number of errors\n",
    "errorsDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Removing section identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errorsDict['section identifiers'] = df_cleaned['sentence'].str.count(pat = r\"^(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)|(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)$\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned['sentence'] = df_cleaned['sentence'].str.replace(pat = r\"^(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)|(S|s|E|e|C|c|T|t|I|i|O|o|N|n){2,}(\\.|,|:|;| ){0,2}[0Oo1Iil!2Z5S6G\\d]{1,2}(. |.| |)$\",\n",
    "                                                            repl = \"\",\n",
    "                                                            regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Removing end of line hyphenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errorsDict['EOL hyphenation'] = df_cleaned['sentence'].str.count(pat = '[-][ ]').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned['sentence'] = df_cleaned['sentence'].str.replace(pat = '[-][ ]',\n",
    "                                                            repl = \"\",\n",
    "                                                            regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Relocating incorrect \"Approved ...\" phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seperate and special method is needed for this match\n",
    "# because the match will be appended to the previous law\n",
    "def replaceInDF(rgx_match: re.Pattern, df: pd.DataFrame, retCount = False):\n",
    "    '''\n",
    "    Find the provided regex pattern in the provided dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rgx_match : re.Pattern\n",
    "        A regular expression pattern that will be search for and replaced in the df\n",
    "    df: pandas.Dataframe\n",
    "        A Pandas dataframe to search and replace for\n",
    "        Should contain a 'sentence' column, in which the matches which will be replaced\n",
    "    retCount: Boolean\n",
    "        A flag to identify whether the function should return the number of matches.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    if retCount == True:\n",
    "        A tuple consisting of:\n",
    "            df: pandas.Dataframe\n",
    "            The modified Dataframe with the matches performed\n",
    "            errorCount: int\n",
    "            A count of how many times this error was found.\n",
    "    else:\n",
    "        df: pandas.Dataframe\n",
    "        The modified Dataframe with the matches performed\n",
    "    '''\n",
    "    \n",
    "    if retCount:\n",
    "        errorCount = 0\n",
    "    \n",
    "    for i in range(0, df.shape[0]):\n",
    "        \n",
    "        # Look for matches\n",
    "        m = re.search(rgx_match, df.iloc[i]['sentence'])\n",
    "\n",
    "        # If matches found then add to the previous sentence\n",
    "        if m:\n",
    "            df.at[i-1, 'sentence'] = df.iloc[i-1]['sentence'] + \" \" + str(m.group())\n",
    "    \n",
    "    \n",
    "        if retCount:\n",
    "            # Remove the matched patterns from sentences\n",
    "            df.at[i, 'sentence'], numError = re.subn(rgx_match, '', df.iloc[i]['sentence'])\n",
    "            errorCount += numError\n",
    "        else:\n",
    "            df.at[i, 'sentence'] = re.sub(rgx_match, '', df.iloc[i]['sentence'])\n",
    "        \n",
    "    if retCount:\n",
    "        return df, errorCount\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx_match = re.compile(\n",
    "    r'^Approved the [0Oo1Iil!2Z5S6G\\d]{1,2}(?:th|st|nd|rd) day of [A-Z][a-z]+, A\\. D\\. .{4}(. |.| |)\\b|Approved [A-Z][a-z]+ [0Oo1Iil!2Z5S6G\\d]{1,2}(?:th|st|nd|rd), A\\. D\\. .{4}(. |.| |)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_cleaned = replaceInDF(rgx_match, df_cleaned, retCount=False)\n",
    "df_cleaned, errorsDict['Approved phrases'] = replaceInDF(rgx_match, df_cleaned, retCount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Removing Act seperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errorsDict['Act seperators'] = df_cleaned['sentence'].str.count(pat = r'^—+(?=\\s*[A-Za-z])').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned['sentence'] = df_cleaned['sentence'].str.replace(pat = r'^—+(?=\\s*[A-Za-z])',\n",
    "                                                            repl = \"\",\n",
    "                                                            regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Removing incorrect numbers at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errorsDict['Incorrect starting nums'] = df_cleaned['sentence'].str.count(pat = r'^[0Oo1Iil!2Z5S6G\\d]{1,3}(. |.| |)').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned['sentence'] = df_cleaned['sentence'].str.replace(pat = r'^[0Oo1Iil!2Z5S6G\\d]{1,3}(. |.| |)',\n",
    "                                                            repl = \"\",\n",
    "                                                            regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Removing session headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "disregarded = 0  # Count for the number removed\n",
    "\n",
    "for i, sent in enumerate(df_cleaned['sentence']):\n",
    "\n",
    "    # If the sentence with \"an\" is found, exit the loop\n",
    "    if 'act' in sent.lower().strip():\n",
    "           break\n",
    "    \n",
    "    # Disregard the sentence since it does not start with \"an\"\n",
    "    df_cleaned.drop(index=i, inplace=True)\n",
    "\n",
    "    disregarded += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences disregarded: 7.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of sentences disregarded: {disregarded}.\")\n",
    "errorsDict['Session headers'] = disregarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Converting to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upperIfNeeded(sentence, ratio = 0.50):\n",
    "    '''\n",
    "    Convert the given sentence list into an uppercase sentence list\n",
    "    if the ratio of uppercase words (not including the ones with a mix of digits \n",
    "    or words like \"SECTION\") to the total words is greater than a fixed value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence: str\n",
    "         A str of sentence to check and convert to uppercase\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        If check is approved the return an uppercase version of str.\n",
    "        Else return the sentence.\n",
    "    '''\n",
    "    \n",
    "    count = 0  # A count of the number of already uppercased words\n",
    "    \n",
    "    for word in sentence.split(\" \"):\n",
    "        # Check whether the word consists of only letters,\n",
    "        # has a length greater than 1, is uppercase, and \n",
    "        # isn't \"SECTION\"\n",
    "        if word.isalpha() and len(word) > 1 and word.isupper() and word != \"SECTION\":\n",
    "            count += 1\n",
    "\n",
    "    # If the count to words ratio is greater\n",
    "    if (count/len(sentence.split(\" \")) > ratio):\n",
    "        # Return all uppercase words\n",
    "        return sentence.upper()        \n",
    "    \n",
    "    # Else, return the original sentence list\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the above function to each sentence in 'org_sent'\n",
    "# And store the output a new column named 'modified'\n",
    "df_cleaned['sentence'] = df_cleaned.apply(lambda x: upperIfNeeded(x['sentence']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results\n",
    "<b>Note:</b> The error fixing above is not perfect and some errors are still present in the dataframe after performing these operations.\n",
    "Also, some errors are too random to match and search using a pattern, and are still present in the dataframe.\n",
    "<br>The output below shows the number of errors corrected for this volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'section identifiers': 528,\n",
       " 'EOL hyphenation': 1826,\n",
       " 'Approved phrases': 114,\n",
       " 'Act seperators': 4,\n",
       " 'Incorrect starting nums': 340,\n",
       " 'Session headers': 7}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## E. Character length\n",
    "Add the character length feature. This is added here because the lengths of the sentences might have changed during the cleaning process above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned[\"length\"] = df_cleaned['sentence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AN ACT TO GRANT TO THE CITY COUNCIL OF CHARLES...</td>\n",
       "      <td>00035</td>\n",
       "      <td>00035</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Be it enacted by the General Assembly of the S...</td>\n",
       "      <td>00035</td>\n",
       "      <td>00036</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The grant herein made is upon the express cond...</td>\n",
       "      <td>00036</td>\n",
       "      <td>00036</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c. 3.</td>\n",
       "      <td>00036</td>\n",
       "      <td>00036</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That in case of the failure of the United Stat...</td>\n",
       "      <td>00036</td>\n",
       "      <td>00036</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence start_page end_page  \\\n",
       "0  AN ACT TO GRANT TO THE CITY COUNCIL OF CHARLES...      00035    00035   \n",
       "1  Be it enacted by the General Assembly of the S...      00035    00036   \n",
       "2  The grant herein made is upon the express cond...      00036    00036   \n",
       "3                                              c. 3.      00036    00036   \n",
       "4  That in case of the failure of the United Stat...      00036    00036   \n",
       "\n",
       "   length  \n",
       "0     179  \n",
       "1     888  \n",
       "2     287  \n",
       "3       5  \n",
       "4     269  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of sentences with a low number of characters as they might not form meaningful sentences.\n",
    "<br>However, first, get the statistics on the length column to avoid removing meaningful sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Define a cutoff for the sentences. All sentences belows this length will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial length\n",
    "ilen = df_cleaned.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the cleaned dataframe:  1359\n",
      "Reduction of about 26.82%\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_cleaned[ df_cleaned[\"length\"] > cut_len]\n",
    "print(\"Length of the cleaned dataframe: \", df_cleaned.shape[0])\n",
    "print(\"Reduction of about {:.2f}%\".format( (1 - df_cleaned.shape[0]/ilen) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "df_cleaned.index.name = \"index\"\n",
    "\n",
    "# Rearrange columns\n",
    "cols = df_cleaned.columns.tolist()\n",
    "cols = [cols[0]] + cols[-1:] + cols[1:-1]\n",
    "df_cleaned = df_cleaned[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F. Adding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adding ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPrefix(fileName: str, nameLen: int) -> str:\n",
    "    '''\n",
    "    Since the fileNames from the excel parsing could be any of any length\n",
    "    (ranging from 1-3), this function appends a string of 0's to the \n",
    "    start of the input so that it is the specified nameLen lengths long.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fileName : str\n",
    "        The file name that needs to be prefixed\n",
    "        The fileName shouldn't have a prefix, such as '.tiff'\n",
    "    nameLen : int\n",
    "        The length of the expected name of the file\n",
    "        Ex. '00034.jpg' would have length of 5\n",
    "        so nameLen should be 5\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A length nameLen file name (prefixed with 0's)\n",
    "    '''\n",
    "    \n",
    "    # prefix_length = nameLen - len(fileName)\n",
    "    prefix = \"0\" * (nameLen - len(fileName))\n",
    "    \n",
    "    return prefix + fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.reset_index(inplace=True)\n",
    "df_cleaned.rename(columns={\"index\" : \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The length of the id of the last row in the dataframe, which is used to assess how many 0's will be prefixed to the other ids\n",
    "maxNumLength = len(str(df_cleaned.last_valid_index()))\n",
    "\n",
    "df_cleaned['id'] = df_cleaned.apply(lambda x: str(year) + \"_\" + addPrefix( str(x['id']), maxNumLength ), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adding the remaining identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.insert(1, 'law_type', 'Acts')\n",
    "df_cleaned.insert(2, 'state', 'SOUTH CAROLINA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the final dataframe to csv for viewing\n",
    "# df_cleaned.to_csv(f\"{year}_faster.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
